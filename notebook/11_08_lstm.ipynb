{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## LSTMã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰\n",
        "è«–æ–‡ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã•ã›ã¦ãƒŠãƒ³ãƒãƒ£ãƒƒãƒ†è«–æ–‡ã‚’latexå½¢å¼ã§å‡ºåŠ›ã•ã›ã‚‹"
      ],
      "metadata": {
        "id": "pVHUGT0PnVAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆ"
      ],
      "metadata": {
        "id": "SiCgS4G5nxHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets tqdm torch requests feedparser --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZf1dSfg2ncZ",
        "outputId": "c2faa827-a618-448c-a9f3-ef89bce96764"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/81.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "doAWRTHmnT-E"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "import glob\n",
        "import re\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import urllib.parse\n",
        "import feedparser\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿"
      ],
      "metadata": {
        "id": "siqtNkzfn8NZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### arXiv API ã‹ã‚‰è«–æ–‡ ID ã‚’ã¾ã¨ã‚ã¦å–å¾—ã™ã‚‹é–¢æ•°"
      ],
      "metadata": {
        "id": "DmpfipBI3Gyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_arxiv_ids(category=\"cs.LG\", max_results=200):\n",
        "    \"\"\"\n",
        "    æŒ‡å®šã‚«ãƒ†ã‚´ãƒªã®æ–°ã—ã„ arXiv è«–æ–‡ ID ã‚’ã¾ã¨ã‚ã¦å–å¾—ã™ã‚‹ç°¡æ˜“é–¢æ•°\n",
        "    ä¾‹: category=\"cs.LG\"ï¼ˆMachine Learningï¼‰\n",
        "    \"\"\"\n",
        "    base_url = \"http://export.arxiv.org/api/query\"\n",
        "\n",
        "    # APIã®ã‚¯ã‚¨ãƒª: ã‚«ãƒ†ã‚´ãƒªæŒ‡å®š + æ–°ã—ã„é †\n",
        "    search_query = f\"cat:{category}\"\n",
        "    params = {\n",
        "        \"search_query\": search_query,\n",
        "        \"start\": 0,\n",
        "        \"max_results\": max_results,\n",
        "        \"sortBy\": \"submittedDate\",\n",
        "        \"sortOrder\": \"descending\",\n",
        "    }\n",
        "\n",
        "    url = base_url + \"?\" + urllib.parse.urlencode(params)\n",
        "    print(\"ğŸ” arXiv API URL:\", url)\n",
        "\n",
        "    feed = feedparser.parse(url)\n",
        "\n",
        "    paper_ids = []\n",
        "    for entry in feed.entries:\n",
        "        # entry.id ä¾‹: \"http://arxiv.org/abs/2401.12345v1\"\n",
        "        m = re.search(r'arxiv.org/abs/(\\d{4}\\.\\d+)', entry.id)\n",
        "        if m:\n",
        "            pid = m.group(1)  # \"2401.12345\"\n",
        "            paper_ids.append(pid)\n",
        "\n",
        "    print(f\"ğŸ“„ å–å¾—ã—ãŸè«–æ–‡IDæ•°: {len(paper_ids)} ä»¶\")\n",
        "    return paper_ids\n",
        "\n",
        "# ã“ã“ã§å¥½ããªã‚«ãƒ†ã‚´ãƒªãƒ»ä»¶æ•°ã‚’æŒ‡å®š\n",
        "# ä¾‹1: cs.LG (Machine Learning) æœ€æ–°ã‹ã‚‰200æœ¬\n",
        "# ä¾‹2: math.PR (Probability)\n",
        "paper_ids = fetch_arxiv_ids(category=\"cs.LG\", max_results=200)\n",
        "\n",
        "print(paper_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8z0Kiju93J5X",
        "outputId": "35aa9e62-f6f2-436b-ae31-0ffd2b1fa659"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” arXiv API URL: http://export.arxiv.org/api/query?search_query=cat%3Acs.LG&start=0&max_results=200&sortBy=submittedDate&sortOrder=descending\n",
            "ğŸ“„ å–å¾—ã—ãŸè«–æ–‡IDæ•°: 200 ä»¶\n",
            "['2511.16674', '2511.16665', '2511.16661', '2511.16655', '2511.16652', '2511.16629', '2511.16622', '2511.16613', '2511.16599', '2511.16597', '2511.16596', '2511.16592', '2511.16587', '2511.16579', '2511.16575', '2511.16573', '2511.16571', '2511.16551', '2511.16550', '2511.16549', '2511.16543', '2511.16540', '2511.16527', '2511.16523', '2511.16520', '2511.16512', '2511.16501', '2511.16483', '2511.16482', '2511.16476', '2511.16475', '2511.16468', '2511.16467', '2511.16445', '2511.16430', '2511.16427', '2511.16426', '2511.16416', '2511.16398', '2511.16377', '2511.16375', '2511.16374', '2511.16373', '2511.16346', '2511.16340', '2511.16333', '2511.16318', '2511.16309', '2511.16297', '2511.16288', '2511.16287', '2511.16258', '2511.16231', '2511.16226', '2511.16225', '2511.16218', '2511.16216', '2511.16207', '2511.16204', '2511.16194', '2511.16192', '2511.16191', '2511.16164', '2511.16158', '2511.16149', '2511.16148', '2511.16145', '2511.16132', '2511.16111', '2511.16105', '2511.16101', '2511.16090', '2511.16087', '2511.16081', '2511.16080', '2511.16073', '2511.16069', '2511.16062', '2511.16061', '2511.16043', '2511.16027', '2511.16026', '2511.16016', '2511.16013', '2511.16006', '2511.15990', '2511.15986', '2511.15983', '2511.15982', '2511.15977', '2511.15969', '2511.15965', '2511.15960', '2511.15941', '2511.15927', '2511.15915', '2511.15906', '2511.15902', '2511.15898', '2511.15884', '2511.15874', '2511.15856', '2511.15854', '2511.15847', '2511.15838', '2511.15822', '2511.15816', '2511.15807', '2511.15779', '2511.15709', '2511.15698', '2511.15694', '2511.15684', '2511.15679', '2511.15661', '2511.15652', '2511.15767', '2511.15634', '2511.15633', '2511.15632', '2511.15619', '2511.15615', '2511.15600', '2511.15543', '2511.15530', '2511.15529', '2511.15522', '2511.15507', '2511.15503', '2511.15487', '2511.15476', '2511.15464', '2511.15454', '2511.15447', '2511.15446', '2511.15445', '2511.15432', '2511.15411', '2511.15409', '2511.15406', '2511.15393', '2511.15375', '2511.15371', '2511.15357', '2511.15350', '2511.15343', '2511.15339', '2511.15332', '2511.15328', '2511.15327', '2511.15324', '2511.15315', '2511.15300', '2511.15276', '2511.15271', '2511.15262', '2511.15256', '2511.15251', '2511.15250', '2511.15248', '2511.15246', '2511.15222', '2511.15210', '2511.15208', '2511.15199', '2511.15196', '2511.15190', '2511.15188', '2511.15183', '2511.15175', '2511.15174', '2511.15173', '2511.15172', '2511.15163', '2511.15162', '2511.15159', '2511.15151', '2511.15146', '2511.15139', '2511.15138', '2511.15137', '2511.15136', '2511.15132', '2511.15125', '2511.15120', '2511.15112', '2511.15083', '2511.15076', '2511.15067', '2511.15062', '2511.15061', '2511.15055', '2511.15048', '2511.15032', '2511.15022', '2511.15015', '2511.15010', '2511.15004', '2511.15003', '2511.15002']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### arXiv ã‚½ãƒ¼ã‚¹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼†å±•é–‹"
      ],
      "metadata": {
        "id": "pFum5tb93p0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"papers\", exist_ok=True)\n",
        "\n",
        "for pid in tqdm(paper_ids, desc=\"Downloading\", ncols=80, ascii=True):\n",
        "    url = f\"https://arxiv.org/src/{pid}v1\"\n",
        "    out_path = f\"papers/{pid}.tar.gz\"\n",
        "    try:\n",
        "        r = requests.get(url, timeout=10)\n",
        "        if r.status_code == 200:\n",
        "            with open(out_path, \"wb\") as f:\n",
        "                f.write(r.content)\n",
        "            # tar / tar.gz / ãã®ä»–ã®åœ§ç¸®å½¢å¼ã«æŸ”è»Ÿã«å¯¾å¿œ\n",
        "            try:\n",
        "                with tarfile.open(out_path, \"r:*\") as tar:\n",
        "                    tar.extractall(f\"papers/{pid}\")\n",
        "            except tarfile.ReadError:\n",
        "                print(f\"Skipped {pid} (not a tar file)\")\n",
        "        else:\n",
        "            print(f\"Skipped {pid} (status {r.status_code})\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {pid}: {e}\")\n"
      ],
      "metadata": {
        "id": "K9mkCl0hnX9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "135be52b-da33-4bed-a227-190e61faf8f4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading:   0%|                                      | 0/200 [00:00<?, ?it/s]/tmp/ipython-input-2326405988.py:14: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(f\"papers/{pid}\")\n",
            "Downloading:  17%|####9                        | 34/200 [00:04<00:15, 11.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.16445 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:  20%|#####8                       | 40/200 [00:05<00:13, 11.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.16416 (not a tar file)\n",
            "Skipped 2511.16398 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:  42%|############                 | 83/200 [00:09<00:13,  8.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.16026 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:  46%|#############1               | 91/200 [00:10<00:09, 11.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.15982 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:  50%|##############3              | 99/200 [00:11<00:10,  9.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.15902 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:  58%|################3           | 117/200 [00:14<00:09,  8.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.15652 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:  66%|##################3         | 131/200 [00:15<00:06, 10.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.15476 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:  80%|######################5     | 161/200 [00:18<00:03, 11.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.15250 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:  92%|#########################6  | 183/200 [00:21<00:02,  7.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.15136 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:  93%|##########################  | 186/200 [00:21<00:01,  9.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.15112 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:  96%|##########################7 | 191/200 [00:22<00:01,  8.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.15067 (not a tar file)\n",
            "Skipped 2511.15062 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: 100%|############################| 200/200 [00:23<00:00,  8.40it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### çµæœç¢ºèª"
      ],
      "metadata": {
        "id": "uHy82Ndv4CZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!find papers -name \"*.tex\" | head -n 20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV_Im0bVwhlz",
        "outputId": "a725ab0b-6ace-4757-f7b5-1dfecfed7647"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "papers/2511.16080/sections/c_subsections/performance_data.tex\n",
            "papers/2511.16080/sections/c_subsections/row_time_data.tex\n",
            "papers/2511.16080/sections/1_introduction.tex\n",
            "papers/2511.16080/sections/3_incremental_resolutions.tex\n",
            "papers/2511.16080/sections/2_ragged_and_named_dimensions.tex\n",
            "papers/2511.16080/sections/6_related_work.tex\n",
            "papers/2511.16080/sections/a_compatibility.tex\n",
            "papers/2511.16080/sections/2_subsections/2B_shapes_and_coordinates.tex\n",
            "papers/2511.16080/sections/2_subsections/2C_arrays.tex\n",
            "papers/2511.16080/sections/2_subsections/2A_dimensions.tex\n",
            "papers/2511.16080/sections/7_conclusion.tex\n",
            "papers/2511.16080/sections/1_subsections/1B_challenges.tex\n",
            "papers/2511.16080/sections/1_subsections/1C_our_design.tex\n",
            "papers/2511.16080/sections/1_subsections/1D_contributions.tex\n",
            "papers/2511.16080/sections/1_subsections/1A_motivating_example.tex\n",
            "papers/2511.16080/sections/5_evaluation.tex\n",
            "papers/2511.16080/sections/c_evaluation.tex\n",
            "papers/2511.16080/sections/4_operon.tex\n",
            "papers/2511.16080/sections/5_subsections/5A_performance.tex\n",
            "papers/2511.16080/sections/5_subsections/5B_limitations.tex\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ãƒ‡ãƒ¼ã‚¿åŠ å·¥"
      ],
      "metadata": {
        "id": "IO3OjIdD7bRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LaTeX æœ¬æ–‡æŠ½å‡º ï¼‹ å‰å‡¦ç†"
      ],
      "metadata": {
        "id": "OSdfMXLInWPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_latex_body(tex_text):\n",
        "    \"\"\"LaTeXæœ¬æ–‡ã‚’æŠ½å‡ºã—ã¦ã‚¯ãƒªãƒ¼ãƒ³åŒ–\"\"\"\n",
        "    text = re.sub(r'%.*', '', tex_text)  # ã‚³ãƒ¡ãƒ³ãƒˆå‰Šé™¤\n",
        "    m = re.search(r'\\\\begin{document}(.*?)\\\\end{document}', text, re.DOTALL)\n",
        "    if m:\n",
        "        text = m.group(1)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "mRvNv8-k-I9L"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re, glob\n",
        "\n",
        "all_texts = []\n",
        "for fname in glob.glob(\"papers/**/*.tex\", recursive=True):\n",
        "    try:\n",
        "        with open(fname, encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            raw = f.read()\n",
        "            cleaned = extract_latex_body(raw)  # â† ã‚ãªãŸã®å®šç¾©ã—ãŸé–¢æ•°\n",
        "            if len(cleaned) > 200:\n",
        "                all_texts.append(cleaned)\n",
        "                # print(f\"æŠ½å‡ºæˆåŠŸ: {fname}\")\n",
        "    except Exception as e:\n",
        "        print(f\"{fname}: {e}\")\n",
        "\n",
        "print(\"æŠ½å‡ºã•ã‚ŒãŸè«–æ–‡æœ¬æ•°:\", len(all_texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c275-ax-9bzn",
        "outputId": "a30a23cc-e5cd-4404-caac-1630995f51de"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "æŠ½å‡ºã•ã‚ŒãŸè«–æ–‡æœ¬æ•°: 817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# all_texts ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½œã‚‹ã¨ã“ã‚ï¼ˆâ†ã“ã‚ŒãŒå…ˆï¼‰\n",
        "\n",
        "TOKEN_PATTERN = r'\\\\[a-zA-Z]+|[{}_^=+\\-\\*/()0-9a-zA-Z]+'\n",
        "SPECIAL_TOKENS = [\"<unk>\", \"<bos>\", \"<eos>\"]\n",
        "\n",
        "tokens = []\n",
        "for txt in all_texts:\n",
        "    # å„è«–æ–‡ã”ã¨ã« <bos>, <eos> ã‚’æŒŸã‚“ã§ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º\n",
        "    body_tokens = re.findall(TOKEN_PATTERN, txt)\n",
        "    tokens.extend([\"<bos>\"] + body_tokens + [\"<eos>\"])\n",
        "\n",
        "print(f\"ãƒˆãƒ¼ã‚¯ãƒ³ç·æ•°: {len(tokens):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLxnu0p7Myb5",
        "outputId": "f2d922bb-5b0a-4505-bd37-c4a2404a30c9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ãƒˆãƒ¼ã‚¯ãƒ³ç·æ•°: 1,986,646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_latex_body(tex_text: str) -> str:\n",
        "    \"\"\"LaTeXæœ¬æ–‡ã‚’æŠ½å‡ºã—ã¦ã‚ã‚‹ç¨‹åº¦ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹ï¼ˆç°¡æ˜“ç‰ˆï¼‰\"\"\"\n",
        "    # ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã‚’å‰Šé™¤ï¼ˆ% ä»¥é™ï¼‰\n",
        "    text = re.sub(r'%.*', '', tex_text)\n",
        "\n",
        "    # \\begin{document}ã€œ\\end{document} ã®é–“ã ã‘ã‚’æŠœãå‡ºã—\n",
        "    m = re.search(r'\\\\begin{document}(.*?)\\\\end{document}', text, re.DOTALL)\n",
        "    if m:\n",
        "        text = m.group(1)\n",
        "\n",
        "    # bibliography / å‚è€ƒæ–‡çŒ®ãƒ»ä»˜éŒ²ä»¥é™ã‚’ã–ã£ãã‚Šå‰Šã‚‹ï¼ˆå¿…è¦ã«å¿œã˜ã¦èª¿æ•´ï¼‰\n",
        "    text = re.split(r'\\\\bibliography|\\\\begin{thebibliography}', text)[0]\n",
        "    text = re.split(r'\\\\appendix', text)[0]\n",
        "\n",
        "    # é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«ã¾ã¨ã‚ã‚‹\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "all_texts = []\n",
        "for fname in glob.glob(\"papers/**/*.tex\", recursive=True):\n",
        "    try:\n",
        "        with open(fname, encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            raw = f.read()\n",
        "        cleaned = extract_latex_body(raw)\n",
        "        if len(cleaned) > 200:  # ã”ãçŸ­ã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯ãƒã‚¤ã‚ºãªã®ã§é™¤å¤–\n",
        "            all_texts.append(cleaned)\n",
        "            #print(f\"xtracted from: {fname}\")\n",
        "    except Exception as e:\n",
        "        print(f\"{fname}: {e}\")\n",
        "\n",
        "print(f\"\\næŠ½å‡ºã•ã‚ŒãŸè«–æ–‡æœ¬æ•°: {len(all_texts)}\")\n",
        "print(f\"ç·ãƒ†ã‚­ã‚¹ãƒˆé•·: {sum(len(t) for t in all_texts):,} æ–‡å­—\")\n",
        "\n",
        "# ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸€ã¤ã«ã¾ã¨ã‚ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ä¿å­˜ã—ã¦ãŠãï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰\n",
        "merged_text = \"\\n\\n\".join(all_texts)\n",
        "with open(\"merged_corpus.tex\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(merged_text)\n",
        "print(\"\\nmerged_corpus.tex ã«ä¿å­˜ã—ã¾ã—ãŸ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dj8HIubk2O2K",
        "outputId": "8fdefadb-d3dc-4056-913d-9545c5a9f0a6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "æŠ½å‡ºã•ã‚ŒãŸè«–æ–‡æœ¬æ•°: 796\n",
            "ç·ãƒ†ã‚­ã‚¹ãƒˆé•·: 10,881,347 æ–‡å­—\n",
            "\n",
            "merged_corpus.tex ã«ä¿å­˜ã—ã¾ã—ãŸ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¨èªå½™è¾æ›¸ä½œæˆ"
      ],
      "metadata": {
        "id": "Hfxny7vSoFQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# SPECIAL_TOKENS ã¯ã•ã£ãã¨åŒã˜ã‚‚ã®ã‚’ä½¿ã†\n",
        "# SPECIAL_TOKENS = [\"<unk>\", \"<bos>\", \"<eos>\"]\n",
        "\n",
        "print(f\"ãƒˆãƒ¼ã‚¯ãƒ³ç·æ•°: {len(tokens):,}\")\n",
        "\n",
        "# èªå½™ã‚’é »åº¦ä¸Šä½ã ã‘ã«åœ§ç¸®\n",
        "counter = Counter(tokens)\n",
        "\n",
        "VOCAB_LIMIT = 40000\n",
        "most_common_tokens = [\n",
        "    w for (w, c) in counter.most_common(VOCAB_LIMIT)\n",
        "    if w not in SPECIAL_TOKENS\n",
        "]\n",
        "\n",
        "vocab = SPECIAL_TOKENS + most_common_tokens\n",
        "\n",
        "word2idx = {w: i for i, w in enumerate(vocab)}\n",
        "idx2word = {i: w for i, w in enumerate(vocab)}\n",
        "\n",
        "UNK_IDX = word2idx[\"<unk>\"]\n",
        "BOS_IDX = word2idx[\"<bos>\"]\n",
        "EOS_IDX = word2idx[\"<eos>\"]\n",
        "\n",
        "print(f\"èªå½™æ•°ï¼ˆåœ§ç¸®å¾Œï¼‰: {len(vocab):,}\")\n",
        "\n",
        "# ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‚’ index ã«å¤‰æ›\n",
        "indexed_tokens = [word2idx.get(t, UNK_IDX) for t in tokens]\n",
        "\n",
        "# å¿µã®ãŸã‚ãƒã‚§ãƒƒã‚¯\n",
        "max_id = max(indexed_tokens)\n",
        "print(\"æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³ID:\", max_id)\n",
        "print(\"èªå½™ã‚µã‚¤ã‚º:\", len(vocab))\n",
        "assert max_id < len(vocab), \"ãƒˆãƒ¼ã‚¯ãƒ³IDãŒèªå½™ã‚µã‚¤ã‚ºã‚’è¶…ãˆã¦ã„ã¾ã™ï¼\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHFzQd_2y9ux",
        "outputId": "4d36f97d-0479-412e-d79f-0ea1f46dc989"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ãƒˆãƒ¼ã‚¯ãƒ³ç·æ•°: 1,986,646\n",
            "èªå½™æ•°ï¼ˆåœ§ç¸®å¾Œï¼‰: 40,001\n",
            "æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³ID: 40000\n",
            "èªå½™ã‚µã‚¤ã‚º: 40001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®šç¾©"
      ],
      "metadata": {
        "id": "SSNzsdoh7p20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Datasetã‚¯ãƒ©ã‚¹å®šç¾©"
      ],
      "metadata": {
        "id": "3EfH4t9CoW1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LatexDataset(Dataset):\n",
        "    \"\"\"å¤§ããªãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‹ã‚‰ã€ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒ«ã‚’åˆ‡ã‚Šå‡ºã™ Dataset\"\"\"\n",
        "    def __init__(self, data, seq_length=60, samples_per_epoch=3000):\n",
        "        self.data = data                      # â† ã™ã§ã« index åŒ–ã•ã‚ŒãŸåˆ—\n",
        "        self.seq_length = seq_length\n",
        "        self.samples_per_epoch = samples_per_epoch\n",
        "\n",
        "        self.max_start = len(self.data) - (self.seq_length + 1)\n",
        "        assert self.max_start > 0, \"ãƒ‡ãƒ¼ã‚¿ãŒçŸ­ã™ãã¾ã™\"\n",
        "\n",
        "    def __len__(self):\n",
        "        # 1epochã‚ãŸã‚Šã®ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’ç›´æ¥æ±ºã‚ã‚‹\n",
        "        return self.samples_per_epoch\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # idx ã¯ä½¿ã‚ãšã€æ¯å›ãƒ©ãƒ³ãƒ€ãƒ ãªä½ç½®ã‹ã‚‰åˆ‡ã‚Šå‡ºã™\n",
        "        start = random.randint(0, self.max_start)\n",
        "        end = start + self.seq_length + 1\n",
        "        chunk = self.data[start:end]\n",
        "\n",
        "        x = torch.tensor(chunk[:-1], dtype=torch.long)  # [T]\n",
        "        y = torch.tensor(chunk[1:], dtype=torch.long)   # [T]\n",
        "        return x, y\n"
      ],
      "metadata": {
        "id": "QEQH22RszXxJ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ã“ã“ã§åˆã‚ã¦ dataset ã‚’ä½œã‚‹ï¼ˆtokens ã§ã¯ãªã indexed_tokens ã‚’æ¸¡ã™ï¼‰\n",
        "seq_length = 60\n",
        "samples_per_epoch = 8000\n",
        "dataset = LatexDataset(indexed_tokens, seq_length=seq_length,\n",
        "                       samples_per_epoch=samples_per_epoch)\n"
      ],
      "metadata": {
        "id": "yQCAnpvHHnUu"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train / Validation ã«åˆ†å‰²ã—ã¦ DataLoader ã‚’ä½œæˆ"
      ],
      "metadata": {
        "id": "I9g7MOlx7G6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio = 0.9\n",
        "n_total = len(dataset)\n",
        "n_train = int(n_total * train_ratio)\n",
        "n_val = n_total - n_train\n",
        "\n",
        "train_ds, val_ds = random_split(dataset, [n_train, n_val])\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"ãƒ‡ãƒ¼ã‚¿æ•°: total={n_total}, train={len(train_ds)}, val={len(val_ds)}\")\n",
        "print(f\"1epoch ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆtrainï¼‰: {len(train_loader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR0X-amZ64rH",
        "outputId": "0d8159e8-f879-4116-b57c-21d010db9a3f"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ãƒ‡ãƒ¼ã‚¿æ•°: total=8000, train=7200, val=800\n",
            "1epoch ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆtrainï¼‰: 225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### å­¦ç¿’"
      ],
      "metadata": {
        "id": "eHHlGpd272Ve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LSTMãƒ¢ãƒ‡ãƒ«å®šç¾©(ç–‘ä¼¼æ•°å­¦è«–æ–‡ç”Ÿæˆ)"
      ],
      "metadata": {
        "id": "6WpX1NvGqTO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTMãƒ¢ãƒ‡ãƒ«å®šç¾©(ç–‘ä¼¼æ•°å­¦è«–æ–‡ç”Ÿæˆ)\n",
        "class LatexTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    LaTeX ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ç”¨ã®ã‚·ãƒ³ãƒ—ãƒ«ãª Transformer è¨€èªãƒ¢ãƒ‡ãƒ«\n",
        "    å…¥åŠ›: x [B, T]  (ãƒˆãƒ¼ã‚¯ãƒ³ID)\n",
        "    å‡ºåŠ›: logits [B, T, V]  (å„ä½ç½®ã”ã¨ã®å˜èªåˆ†å¸ƒã®ãƒ­ã‚¸ãƒƒãƒˆ)\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 vocab_size: int,\n",
        "                 d_model: int = 256,          # åŸ‹ã‚è¾¼ã¿ï¼†éš ã‚Œæ¬¡å…ƒ\n",
        "                 nhead: int = 4,              # ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰æ³¨æ„ã®ãƒ˜ãƒƒãƒ‰æ•°\n",
        "                 num_layers: int = 4,         # Transformer å±¤æ•°\n",
        "                 dim_feedforward: int = 512,  # FFN ã®ä¸­é–“æ¬¡å…ƒ\n",
        "                 dropout: float = 0.1,\n",
        "                 max_seq_len: int = 512):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        # èªå½™åŸ‹ã‚è¾¼ã¿\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # ä½ç½®åŸ‹ã‚è¾¼ã¿ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãª learnable embeddingï¼‰\n",
        "        self.pos_embedding = nn.Embedding(max_seq_len, d_model)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True  # [B, T, E] ã§å‡¦ç†\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "\n",
        "        # å‡ºåŠ›å±¤\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "        # Causal mask ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨ã«ä¿æŒ\n",
        "        self.register_buffer(\"mask\", None, persistent=False)\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz: int, device: torch.device):\n",
        "        \"\"\"\n",
        "        è‡ªå·±å›å¸°ç”¨ã®ãƒã‚¹ã‚¯ (æœªæ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¦‹ãªã„ã‚ˆã†ã«ã™ã‚‹)\n",
        "        shape: [T, T]\n",
        "        \"\"\"\n",
        "        if (self.mask is None) or (self.mask.size(0) != sz):\n",
        "            mask = torch.full((sz, sz), float(\"-inf\"), device=device)\n",
        "            mask = torch.triu(mask, diagonal=1)\n",
        "            self.mask = mask\n",
        "        return self.mask\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: [B, T]  ãƒˆãƒ¼ã‚¯ãƒ³IDåˆ—\n",
        "        æˆ»ã‚Šå€¤:\n",
        "            logits: [B, T, V]\n",
        "        \"\"\"\n",
        "        B, T = x.size()\n",
        "        device = x.device\n",
        "\n",
        "        # ä½ç½®ID [0, 1, ..., T-1]\n",
        "        positions = torch.arange(T, device=device).unsqueeze(0).expand(B, T)  # [B, T]\n",
        "\n",
        "        tok_emb = self.token_embedding(x)          # [B, T, d_model]\n",
        "        pos_emb = self.pos_embedding(positions)    # [B, T, d_model]\n",
        "        h = tok_emb + pos_emb                      # [B, T, d_model]\n",
        "\n",
        "        # causal mask (æœªæ¥ã®æƒ…å ±ã‚’è¦‹ãªã„)\n",
        "        src_mask = self._generate_square_subsequent_mask(T, device=device)  # [T, T]\n",
        "\n",
        "        # TransformerEncoder ã«é€šã™\n",
        "        # batch_first=True ãªã®ã§ h: [B, T, d_model], mask ã¯ [T, T]\n",
        "        h = self.transformer(h, mask=src_mask)     # [B, T, d_model]\n",
        "\n",
        "        logits = self.fc_out(h)                    # [B, T, V]\n",
        "        return logits"
      ],
      "metadata": {
        "id": "h7m2BKKEobY6"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ç¹°ã‚Šè¿”ã—è¨ˆç®—"
      ],
      "metadata": {
        "id": "NKqEGskKqeEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"device: {device}\")\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print(\"vocab_size:\", vocab_size)\n",
        "\n",
        "model = LatexLSTM(vocab_size=vocab_size).to(device)\n",
        "\n",
        "print(\"model.fc.out_features:\", model.fc.out_features)\n",
        "assert model.fc.out_features == vocab_size, \"ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›æ¬¡å…ƒã¨èªå½™æ•°ãŒä¸€è‡´ã—ã¦ã„ã¾ã›ã‚“\"\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "num_epochs = 30\n",
        "max_grad_norm = 1.0  # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã®æœ€å¤§ãƒãƒ«ãƒ \n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # -------------------------------\n",
        "    # Train\n",
        "    # -------------------------------\n",
        "    model.train()\n",
        "    total_train_loss = 0.0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        # x, y: [B, T]\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)  # [B, T, V]\n",
        "\n",
        "        # CrossEntropyLoss ã¯ [N, C] vs [N] ã‚’ã¨ã‚‹ã®ã§ reshape ã™ã‚‹\n",
        "        loss = criterion(\n",
        "            logits.view(-1, logits.size(-1)),  # [B*T, V]\n",
        "            y.view(-1)                         # [B*T]\n",
        "        )\n",
        "        loss.backward()\n",
        "\n",
        "        # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆå‹¾é…çˆ†ç™ºå¯¾ç­–ï¼‰\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Validation\n",
        "    # -------------------------------\n",
        "    model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            val_loss = criterion(\n",
        "                logits.view(-1, logits.size(-1)),\n",
        "                y.view(-1)\n",
        "            )\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "    # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’é€²ã‚ã‚‹\n",
        "    scheduler.step()\n",
        "\n",
        "    print(\n",
        "        f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "        f\"train_loss={avg_train_loss:.4f}  val_loss={avg_val_loss:.4f}  \"\n",
        "        f\"lr={scheduler.get_last_lr()[0]:.5f}\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMzBAuxWqcea",
        "outputId": "9b9521ae-4cea-48cb-8ac7-543ba51510e6"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n",
            "vocab_size: 40001\n",
            "model.fc.out_features: 40001\n",
            "Epoch [1/30] train_loss=8.0923  val_loss=7.9161  lr=0.00100\n",
            "Epoch [2/30] train_loss=7.9019  val_loss=7.8266  lr=0.00100\n",
            "Epoch [3/30] train_loss=7.8582  val_loss=7.8477  lr=0.00100\n",
            "Epoch [4/30] train_loss=7.8564  val_loss=7.8331  lr=0.00100\n",
            "Epoch [5/30] train_loss=7.8176  val_loss=7.8182  lr=0.00100\n",
            "Epoch [6/30] train_loss=7.6932  val_loss=7.5661  lr=0.00100\n",
            "Epoch [7/30] train_loss=7.3387  val_loss=6.9298  lr=0.00100\n",
            "Epoch [8/30] train_loss=6.7144  val_loss=6.5227  lr=0.00100\n",
            "Epoch [9/30] train_loss=6.3712  val_loss=6.1914  lr=0.00100\n",
            "Epoch [10/30] train_loss=6.1351  val_loss=6.0282  lr=0.00050\n",
            "Epoch [11/30] train_loss=5.9350  val_loss=5.8018  lr=0.00050\n",
            "Epoch [12/30] train_loss=5.8572  val_loss=5.6949  lr=0.00050\n",
            "Epoch [13/30] train_loss=5.7490  val_loss=5.6639  lr=0.00050\n",
            "Epoch [14/30] train_loss=5.6706  val_loss=5.5697  lr=0.00050\n",
            "Epoch [15/30] train_loss=5.6134  val_loss=5.5419  lr=0.00050\n",
            "Epoch [16/30] train_loss=5.5502  val_loss=5.4215  lr=0.00050\n",
            "Epoch [17/30] train_loss=5.4730  val_loss=5.3013  lr=0.00050\n",
            "Epoch [18/30] train_loss=5.4189  val_loss=5.3797  lr=0.00050\n",
            "Epoch [19/30] train_loss=5.3416  val_loss=5.2653  lr=0.00050\n",
            "Epoch [20/30] train_loss=5.3186  val_loss=5.2518  lr=0.00025\n",
            "Epoch [21/30] train_loss=5.2519  val_loss=5.1407  lr=0.00025\n",
            "Epoch [22/30] train_loss=5.2282  val_loss=5.0980  lr=0.00025\n",
            "Epoch [23/30] train_loss=5.2006  val_loss=5.0878  lr=0.00025\n",
            "Epoch [24/30] train_loss=5.1660  val_loss=5.0386  lr=0.00025\n",
            "Epoch [25/30] train_loss=5.1361  val_loss=5.0401  lr=0.00025\n",
            "Epoch [26/30] train_loss=5.1114  val_loss=5.0688  lr=0.00025\n",
            "Epoch [27/30] train_loss=5.1039  val_loss=4.9816  lr=0.00025\n",
            "Epoch [28/30] train_loss=5.0511  val_loss=4.9929  lr=0.00025\n",
            "Epoch [29/30] train_loss=5.0664  val_loss=4.9523  lr=0.00025\n",
            "Epoch [30/30] train_loss=5.0341  val_loss=4.8560  lr=0.00013\n",
            "CPU times: user 10min 25s, sys: 5.19 s, total: 10min 30s\n",
            "Wall time: 10min 35s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### äºˆæ¸¬"
      ],
      "metadata": {
        "id": "5H6pGlea8C3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç”¨ãƒ˜ãƒ«ãƒ‘ãƒ¼\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fDyXRUFKolPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_from_logits(logits: torch.Tensor,\n",
        "                       temperature: float = 0.8,\n",
        "                       top_k: int = 50) -> int:\n",
        "    \"\"\"\n",
        "    logits: [V] ã®1æ™‚åˆ»åˆ†ã®ãƒ­ã‚¸ãƒƒãƒˆ\n",
        "    æˆ»ã‚Šå€¤: æ¬¡ã«é¸ã¶ãƒˆãƒ¼ã‚¯ãƒ³ID\n",
        "    \"\"\"\n",
        "    # æ¸©åº¦ã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
        "    logits = logits / temperature\n",
        "\n",
        "    # top-k ã ã‘æ®‹ã—ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆæ¥µç«¯ãªãƒã‚¤ã‚ºã‚’æ¸›ã‚‰ã™ï¼‰\n",
        "    if top_k is not None and top_k > 0:\n",
        "        values, indices = torch.topk(logits, top_k)\n",
        "        probs = torch.softmax(values, dim=-1)\n",
        "        idx_in_topk = torch.multinomial(probs, 1).item()\n",
        "        next_token_id = indices[idx_in_topk].item()\n",
        "        return next_token_id\n",
        "    else:\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        next_token_id = torch.multinomial(probs, 1).item()\n",
        "        return next_token_id\n"
      ],
      "metadata": {
        "id": "Rti0zCt5owOo"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆé–¢æ•°"
      ],
      "metadata": {
        "id": "A64q9CmRqu_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(seed_text, length=200, temperature=0.8, top_k=50, seq_length=60):\n",
        "    model.eval()\n",
        "    words = re.findall(TOKEN_PATTERN, seed_text)\n",
        "\n",
        "    for _ in range(length):\n",
        "        # ğŸ”¥ ç›´è¿‘ seq_length åˆ†ã ã‘ä½¿ã†\n",
        "        seq = [word2idx.get(w, UNK_IDX) for w in words[-seq_length:]]\n",
        "\n",
        "        x = torch.tensor([seq], dtype=torch.long).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)[0, -1]  # ãƒ©ã‚¹ãƒˆæ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ã®ã¿\n",
        "\n",
        "            # <unk> ã‚’å‡ºã«ããã™ã‚‹\n",
        "            logits[UNK_IDX] -= 10.0\n",
        "\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # top-k ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆæ¨å¥¨ï¼‰\n",
        "            values, indices = torch.topk(logits, top_k)\n",
        "            probs = torch.softmax(values, dim=0)\n",
        "            next_idx = indices[torch.multinomial(probs, 1).item()].item()\n",
        "\n",
        "        words.append(idx2word[next_idx])\n",
        "\n",
        "    return \" \".join(words)\n"
      ],
      "metadata": {
        "id": "j5K4ik0Bpa3V"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚µãƒ³ãƒ—ãƒ«"
      ],
      "metadata": {
        "id": "TUKHVRGPrHT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = r\"\\section{Introduction} We consider the problem of minimizing\"\n",
        "print(\"\\nç”Ÿæˆçµæœ:\\n\")\n",
        "text = generate(seed, length=200, temperature=0.7, top_k=30, seq_length=seq_length)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcpQbqcTrAsa",
        "outputId": "2d813ada-21c8-43c5-b960-324367e68fe3"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ç”Ÿæˆçµæœ:\n",
            "\n",
            "\\section {Introduction} We consider the problem of minimizing the following approximation we also show that the following lemma in the form of the model of the same set of the initial system \\begin {equation} \\label {eq erm-ineq-rearranged} \\begin {split} \\cA _{ \\text {train}} \\times M} \\text {real}} \\quad \\forall i \\in {1 \\dots \\mathcal K_{t x}f } \\end {aligned} \\end {equation} where \\mathcal {T} and \\mathbf {x} = \\mathbf {y}_i \\mathbf { \\theta _{ \\boldsymbol {x}_t \\boldsymbol { \\mu } \\boldsymbol { \\alpha } \\STATE Calculate \\boldsymbol {x}_t \\boldsymbol { \\Omega } \\STATE Calculate \\boldsymbol {m}^{*} \\boldsymbol {s}_i) = \\boldsymbol { \\boldsymbol {x}_0 \\STATE \\textbf { \\textbf { \\boldsymbol {0} \\boldsymbol { \\boldsymbol {c}}_{ \\boldsymbol {x}) \\STATE Calculate \\boldsymbol {n} \\boldsymbol {x}_t \\boldsymbol {x}) = \\boldsymbol { \\mathbf {0}}(t+1) - \\boldsymbol { \\boldsymbol {X}} \\boldsymbol { \\theta } \\boldsymbol {x}) \\STATE Collect \\boldsymbol { \\boldsymbol { \\boldsymbol {v}}^{t+1} \\boldsymbol { \\boldsymbol {y}} \\boldsymbol { \\boldsymbol {x}} \\boldsymbol { \\boldsymbol {x}}_{ \\boldsymbol { \\theta } = \\mathbf {I} - \\mathbf {x}_i \\end {aligned} \\end {align*} where the following inequality is that the function function is the number of samples in the target space and the state matrix ( \\mathbf {F}_ \\mathbf {n} \\mathbf {N} \\mathbf {x}) ) is a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# LaTeX â†’ PDF å¤‰æ› (Google Colab / Linux ç’°å¢ƒç”¨)\n",
        "# =====================================================\n",
        "\n",
        "import os, subprocess\n",
        "\n",
        "# â‘  ç”Ÿæˆæ¸ˆã¿LaTeXæœ¬æ–‡ã‚’ã“ã“ã«è²¼ã‚‹ï¼ˆæ•´å½¢å¾Œã®textã‚’å…¥ã‚Œã‚‹ï¼‰\n",
        "latex_code = r\"\"\"\n",
        "\\documentclass[11pt]{article}\n",
        "\n",
        "% å¿…è¦æœ€ä½é™ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸\n",
        "\\usepackage{amsmath, amssymb, amsfonts}\n",
        "\\usepackage{bm}\n",
        "\\usepackage[margin=25mm]{geometry}\n",
        "\n",
        "\\begin{document}\n",
        "\n",
        "\\section{Introduction}\n",
        "\n",
        "We consider the problem of minimizing an approximation function under\n",
        "a general learning framework. In this work, we introduce a unified model\n",
        "that aims to improve prediction performance and enhance optimization stability\n",
        "across a variety of datasets.\n",
        "\n",
        "To study the theoretical properties of our model, we begin by examining\n",
        "a key lemma related to empirical risk minimization. A simplified form\n",
        "of the proposed inequality can be written as\n",
        "\\begin{equation}\n",
        "    \\mathcal{A}_{\\text{train}}(x)\n",
        "    \\leq \\mathcal{A}_{\\text{test}}(x) + \\epsilon,\n",
        "\\end{equation}\n",
        "where $\\epsilon$ is a small constant representing the empirical gap between\n",
        "training and test distributions.\n",
        "\n",
        "Furthermore, we consider the iterative update process of the model parameters.\n",
        "For example, the next state $\\mathbf{x}_{t+1}$ may be approximated by\n",
        "\\begin{equation}\n",
        "    \\mathbf{x}_{t+1}\n",
        "    = \\mathbf{x}_{t} - \\eta \\, \\nabla_{\\mathbf{x}} \\mathcal{L}(\\mathbf{x}_t),\n",
        "\\end{equation}\n",
        "where $\\eta$ is a learning rate and $\\mathcal{L}$ denotes the loss function.\n",
        "This provides a generic template for gradient-based optimization in our framework.\n",
        "\n",
        "The main contributions of this paper can be summarized as follows:\n",
        "\\begin{itemize}\n",
        "    \\item We propose a unified learning framework applicable to various tasks.\n",
        "    \\item We provide theoretical insights into the stability of the proposed model.\n",
        "    \\item We demonstrate competitive performance across multiple datasets.\n",
        "\\end{itemize}\n",
        "\n",
        "\\section{Conclusion}\n",
        "\n",
        "The generated LaTeX text has been reformatted so that it can be compiled\n",
        "without errors. Although the original automatically generated expressions\n",
        "were incomplete, we replaced them with valid representative examples\n",
        "to ensure successful PDF generation.\n",
        "\n",
        "\\end{document}\n",
        "\"\"\"\n",
        "\n",
        "# â‘¡ texãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜\n",
        "with open(\"generated.tex\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(latex_code)\n",
        "\n",
        "# â‘¢ LaTeXã‚³ãƒãƒ³ãƒ‰ãŒä½¿ãˆã‚‹ã‚ˆã†ã«TexLiveã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!apt-get update -qq\n",
        "!apt-get install -y texlive-latex-base texlive-latex-extra texlive-fonts-recommended > /dev/null\n",
        "\n",
        "# â‘£ pdflatexã§PDFã«å¤‰æ›\n",
        "!pdflatex -interaction=nonstopmode generated.tex > /dev/null\n",
        "\n",
        "# â‘¤ å‡ºåŠ›ç¢ºèª\n",
        "print(\"PDFç”Ÿæˆå®Œäº†: generated.pdf\")\n",
        "!ls -lh generated.pdf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZMXytkVrvnL",
        "outputId": "007b3a80-a84c-4b7f-924c-4f1a93c5922a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "\n",
            "kpathsea: Running mktexpk --mfmode / --bdpi 600 --mag 1+0/600 --dpi 600 tcrm1095\n",
            "mktexpk: Running mf-nowin -progname=mf \\mode:=ljfour; mag:=1+0/600; nonstopmode; input tcrm1095\n",
            "This is METAFONT, Version 2.71828182 (TeX Live 2022/dev/Debian) (preloaded base=mf)\n",
            "\n",
            "(/usr/share/texlive/texmf-dist/fonts/source/jknappen/ec/tcrm1095.mf\n",
            "(/usr/share/texlive/texmf-dist/fonts/source/jknappen/ec/exbase.mf)\n",
            "(/usr/share/texlive/texmf-dist/fonts/source/jknappen/ec/tcrm.mf\n",
            "(/usr/share/texlive/texmf-dist/fonts/source/jknappen/ec/txsymb.mf\n",
            " Ok (/usr/share/texlive/texmf-dist/fonts/source/jknappen/ec/exaccess.mf\n",
            " Ok) (/usr/share/texlive/texmf-dist/fonts/source/jknappen/ec/txpseudo.mf\n",
            " Ok) (/usr/share/texlive/texmf-dist/fonts/source/jknappen/ec/txaccent.mf\n",
            " Ok [0] [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [27] [29])\n",
            "(/usr/share/texlive/texmf-dist/fonts/source/jknappen/ec/txgen.mf\n",
            " Ok [100] [109] [98] [99] [108])\n",
            "(/usr/share/texlive/texmf-dist/fonts/source/jknappen/ec/txsymbol.mf\n",
            " Ok [13] [18] [21] [22] [23] [24] [25] [26] [28] [31] [32] [36] [39] [44]\n",
            "[45] [46] [42] [47] [60] [61] [62] [77] [79] [87] [110] [91] [93] [94] [95]\n",
            "[96] [126] [127] [128] [129] [130] [131] [132] [133] [134] [135] [136] [137]\n",
            "[138] [139] [140] [141] [142] [143] [144] [145] [146] [147] [148] [149]\n",
            "[150] [151] [152] [153] [154] [155] [156] [157] [158] [159] [160] [161]\n",
            "[162] [163] [164] [165] [166] [167] [168] [169] [171] [172] [173] [174]\n",
            "[175] [177] [176] [180] [181] [182] [183] [184] [187] [191] [214] [246])\n",
            "(/usr/share/texlive/texmf-dist/fonts/source/jknappen/ec/txromod.mf\n",
            " Ok [48] [49] [50] [51] [52] [53] [54] [55] [56] [57])\n",
            "(/usr/share/texlive/texmf-dist/fonts/source/jknappen/ec/txrsuper.mf\n",
            " Ok [185] [178] [179] [170] [186])\n",
            "(/usr/share/texlive/texmf-dist/fonts/source/jknappen/ec/txrfract.mf\n",
            " Ok [188] [189] [190]) ) ) )\n",
            "(some charht values had to be adjusted by as much as 0.06952pt)\n",
            "Font metrics written on tcrm1095.tfm.\n",
            "Output written on tcrm1095.600gf (128 characters, 25592 bytes).\n",
            "Transcript written on tcrm1095.log.\n",
            "mktexpk: /root/.texlive2021/texmf-var/fonts/pk/ljfour/jknappen/ec/tcrm1095.600pk: successfully generated.\n",
            "PDFç”Ÿæˆå®Œäº†: generated.pdf\n",
            "-rw-r--r-- 1 root root 82K Nov 23 05:17 generated.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L12fyJHI-8KI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}