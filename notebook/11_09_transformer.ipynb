{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Transformerã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰\n",
        "è«–æ–‡ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã•ã›ã¦ãƒŠãƒ³ãƒãƒ£ãƒƒãƒ†è«–æ–‡ã‚’latexå½¢å¼ã§å‡ºåŠ›ã•ã›ã‚‹<br>\n",
        "Transformenrç‰ˆ"
      ],
      "metadata": {
        "id": "pVHUGT0PnVAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆ"
      ],
      "metadata": {
        "id": "SiCgS4G5nxHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets tqdm torch requests feedparser --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZf1dSfg2ncZ",
        "outputId": "98d641e2-7332-421d-fd86-ce98e3a8c252"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "doAWRTHmnT-E"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "import glob\n",
        "import re\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import urllib.parse\n",
        "import feedparser\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿"
      ],
      "metadata": {
        "id": "siqtNkzfn8NZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### arXiv API ã‹ã‚‰è«–æ–‡ ID ã‚’ã¾ã¨ã‚ã¦å–å¾—ã™ã‚‹é–¢æ•°"
      ],
      "metadata": {
        "id": "DmpfipBI3Gyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_arxiv_ids(category=\"cs.LG\", max_results=200):\n",
        "    \"\"\"\n",
        "    æŒ‡å®šã‚«ãƒ†ã‚´ãƒªã®æ–°ã—ã„ arXiv è«–æ–‡ ID ã‚’ã¾ã¨ã‚ã¦å–å¾—ã™ã‚‹ç°¡æ˜“é–¢æ•°\n",
        "    ä¾‹: category=\"cs.LG\"ï¼ˆMachine Learningï¼‰\n",
        "    \"\"\"\n",
        "    base_url = \"http://export.arxiv.org/api/query\"\n",
        "\n",
        "    # APIã®ã‚¯ã‚¨ãƒª: ã‚«ãƒ†ã‚´ãƒªæŒ‡å®š + æ–°ã—ã„é †\n",
        "    search_query = f\"cat:{category}\"\n",
        "    params = {\n",
        "        \"search_query\": search_query,\n",
        "        \"start\": 0,\n",
        "        \"max_results\": max_results,\n",
        "        \"sortBy\": \"submittedDate\",\n",
        "        \"sortOrder\": \"descending\",\n",
        "    }\n",
        "\n",
        "    url = base_url + \"?\" + urllib.parse.urlencode(params)\n",
        "    print(\"ğŸ” arXiv API URL:\", url)\n",
        "\n",
        "    feed = feedparser.parse(url)\n",
        "\n",
        "    paper_ids = []\n",
        "    for entry in feed.entries:\n",
        "        # entry.id ä¾‹: \"http://arxiv.org/abs/2401.12345v1\"\n",
        "        m = re.search(r'arxiv.org/abs/(\\d{4}\\.\\d+)', entry.id)\n",
        "        if m:\n",
        "            pid = m.group(1)  # \"2401.12345\"\n",
        "            paper_ids.append(pid)\n",
        "\n",
        "    print(f\"ğŸ“„ å–å¾—ã—ãŸè«–æ–‡IDæ•°: {len(paper_ids)} ä»¶\")\n",
        "    return paper_ids\n",
        "\n",
        "# ã“ã“ã§å¥½ããªã‚«ãƒ†ã‚´ãƒªãƒ»ä»¶æ•°ã‚’æŒ‡å®š\n",
        "# ä¾‹1: cs.LG (Machine Learning) æœ€æ–°ã‹ã‚‰200æœ¬\n",
        "# ä¾‹2: math.PR (Probability)\n",
        "paper_ids = fetch_arxiv_ids(category=\"cs.LG\", max_results=200)\n",
        "\n",
        "print(paper_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8z0Kiju93J5X",
        "outputId": "18a32741-20d1-4fc9-dda0-8d340e28dea8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” arXiv API URL: http://export.arxiv.org/api/query?search_query=cat%3Acs.LG&start=0&max_results=200&sortBy=submittedDate&sortOrder=descending\n",
            "ğŸ“„ å–å¾—ã—ãŸè«–æ–‡IDæ•°: 200 ä»¶\n",
            "['2511.16674', '2511.16665', '2511.16661', '2511.16655', '2511.16652', '2511.16629', '2511.16622', '2511.16613', '2511.16599', '2511.16597', '2511.16596', '2511.16592', '2511.16587', '2511.16579', '2511.16575', '2511.16573', '2511.16571', '2511.16551', '2511.16550', '2511.16549', '2511.16543', '2511.16540', '2511.16527', '2511.16523', '2511.16520', '2511.16512', '2511.16501', '2511.16483', '2511.16482', '2511.16476', '2511.16475', '2511.16468', '2511.16467', '2511.16445', '2511.16430', '2511.16427', '2511.16426', '2511.16416', '2511.16398', '2511.16377', '2511.16375', '2511.16374', '2511.16373', '2511.16346', '2511.16340', '2511.16333', '2511.16318', '2511.16309', '2511.16297', '2511.16288', '2511.16287', '2511.16258', '2511.16231', '2511.16226', '2511.16225', '2511.16218', '2511.16216', '2511.16207', '2511.16204', '2511.16194', '2511.16192', '2511.16191', '2511.16164', '2511.16158', '2511.16149', '2511.16148', '2511.16145', '2511.16132', '2511.16111', '2511.16105', '2511.16101', '2511.16090', '2511.16087', '2511.16081', '2511.16080', '2511.16073', '2511.16069', '2511.16062', '2511.16061', '2511.16043', '2511.16027', '2511.16026', '2511.16016', '2511.16013', '2511.16006', '2511.15990', '2511.15986', '2511.15983', '2511.15982', '2511.15977', '2511.15969', '2511.15965', '2511.15960', '2511.15941', '2511.15927', '2511.15915', '2511.15906', '2511.15902', '2511.15898', '2511.15884', '2511.15874', '2511.15856', '2511.15854', '2511.15847', '2511.15838', '2511.15822', '2511.15816', '2511.15807', '2511.15779', '2511.15709', '2511.15698', '2511.15694', '2511.15684', '2511.15679', '2511.15661', '2511.15652', '2511.15767', '2511.15634', '2511.15633', '2511.15632', '2511.15619', '2511.15615', '2511.15600', '2511.15543', '2511.15530', '2511.15529', '2511.15522', '2511.15507', '2511.15503', '2511.15487', '2511.15476', '2511.15464', '2511.15454', '2511.15447', '2511.15446', '2511.15445', '2511.15432', '2511.15411', '2511.15409', '2511.15406', '2511.15393', '2511.15375', '2511.15371', '2511.15357', '2511.15350', '2511.15343', '2511.15339', '2511.15332', '2511.15328', '2511.15327', '2511.15324', '2511.15315', '2511.15300', '2511.15276', '2511.15271', '2511.15262', '2511.15256', '2511.15251', '2511.15250', '2511.15248', '2511.15246', '2511.15222', '2511.15210', '2511.15208', '2511.15199', '2511.15196', '2511.15190', '2511.15188', '2511.15183', '2511.15175', '2511.15174', '2511.15173', '2511.15172', '2511.15163', '2511.15162', '2511.15159', '2511.15151', '2511.15146', '2511.15139', '2511.15138', '2511.15137', '2511.15136', '2511.15132', '2511.15125', '2511.15120', '2511.15112', '2511.15083', '2511.15076', '2511.15067', '2511.15062', '2511.15061', '2511.15055', '2511.15048', '2511.15032', '2511.15022', '2511.15015', '2511.15010', '2511.15004', '2511.15003', '2511.15002']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### arXiv ã‚½ãƒ¼ã‚¹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼†å±•é–‹"
      ],
      "metadata": {
        "id": "pFum5tb93p0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"papers\", exist_ok=True)\n",
        "\n",
        "for pid in tqdm(paper_ids, desc=\"Downloading\", ncols=80, ascii=True):\n",
        "    url = f\"https://arxiv.org/src/{pid}v1\"\n",
        "    out_path = f\"papers/{pid}.tar.gz\"\n",
        "    try:\n",
        "        r = requests.get(url, timeout=10)\n",
        "        if r.status_code == 200:\n",
        "            with open(out_path, \"wb\") as f:\n",
        "                f.write(r.content)\n",
        "            # tar / tar.gz / ãã®ä»–ã®åœ§ç¸®å½¢å¼ã«æŸ”è»Ÿã«å¯¾å¿œ\n",
        "            try:\n",
        "                with tarfile.open(out_path, \"r:*\") as tar:\n",
        "                    tar.extractall(f\"papers/{pid}\")\n",
        "            except tarfile.ReadError:\n",
        "                print(f\"Skipped {pid} (not a tar file)\")\n",
        "        else:\n",
        "            print(f\"Skipped {pid} (status {r.status_code})\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {pid}: {e}\")\n"
      ],
      "metadata": {
        "id": "K9mkCl0hnX9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26377f11-c275-45ba-ef28-7f11013bb81d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading:   0%|                                      | 0/200 [00:00<?, ?it/s]/tmp/ipython-input-2326405988.py:14: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(f\"papers/{pid}\")\n",
            "Downloading:  17%|####9                        | 34/200 [00:04<00:16, 10.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.16445 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:  20%|#####8                       | 40/200 [00:04<00:14, 11.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.16416 (not a tar file)\n",
            "Skipped 2511.16398 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:  42%|############                 | 83/200 [00:09<00:14,  8.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.16026 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:  45%|#############                | 90/200 [00:10<00:09, 11.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.15982 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:  49%|##############2              | 98/200 [00:11<00:10,  9.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.15902 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:  59%|################5           | 118/200 [00:13<00:08,  9.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.15652 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:  66%|##################3         | 131/200 [00:14<00:06, 10.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.15476 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:  80%|######################4     | 160/200 [00:17<00:04,  9.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.15250 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:  92%|#########################6  | 183/200 [00:20<00:01, 12.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.15136 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading:  92%|#########################9  | 185/200 [00:20<00:01, 12.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.15112 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:  96%|##########################7 | 191/200 [00:20<00:00,  9.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.15067 (not a tar file)\n",
            "Skipped 2511.15062 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: 100%|############################| 200/200 [00:22<00:00,  8.87it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### çµæœç¢ºèª"
      ],
      "metadata": {
        "id": "uHy82Ndv4CZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!find papers -name \"*.tex\" | head -n 20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV_Im0bVwhlz",
        "outputId": "df11b756-7518-4062-fe4b-97ba1d6b233a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "papers/2511.16080/sections/c_subsections/performance_data.tex\n",
            "papers/2511.16080/sections/c_subsections/row_time_data.tex\n",
            "papers/2511.16080/sections/1_introduction.tex\n",
            "papers/2511.16080/sections/3_incremental_resolutions.tex\n",
            "papers/2511.16080/sections/2_ragged_and_named_dimensions.tex\n",
            "papers/2511.16080/sections/6_related_work.tex\n",
            "papers/2511.16080/sections/a_compatibility.tex\n",
            "papers/2511.16080/sections/2_subsections/2B_shapes_and_coordinates.tex\n",
            "papers/2511.16080/sections/2_subsections/2C_arrays.tex\n",
            "papers/2511.16080/sections/2_subsections/2A_dimensions.tex\n",
            "papers/2511.16080/sections/7_conclusion.tex\n",
            "papers/2511.16080/sections/1_subsections/1B_challenges.tex\n",
            "papers/2511.16080/sections/1_subsections/1C_our_design.tex\n",
            "papers/2511.16080/sections/1_subsections/1D_contributions.tex\n",
            "papers/2511.16080/sections/1_subsections/1A_motivating_example.tex\n",
            "papers/2511.16080/sections/5_evaluation.tex\n",
            "papers/2511.16080/sections/c_evaluation.tex\n",
            "papers/2511.16080/sections/4_operon.tex\n",
            "papers/2511.16080/sections/5_subsections/5A_performance.tex\n",
            "papers/2511.16080/sections/5_subsections/5B_limitations.tex\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ãƒ‡ãƒ¼ã‚¿åŠ å·¥"
      ],
      "metadata": {
        "id": "IO3OjIdD7bRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LaTeX æœ¬æ–‡æŠ½å‡º ï¼‹ å‰å‡¦ç†"
      ],
      "metadata": {
        "id": "OSdfMXLInWPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_latex_body(tex_text):\n",
        "    \"\"\"LaTeXæœ¬æ–‡ã‚’æŠ½å‡ºã—ã¦ã‚¯ãƒªãƒ¼ãƒ³åŒ–\"\"\"\n",
        "    text = re.sub(r'%.*', '', tex_text)  # ã‚³ãƒ¡ãƒ³ãƒˆå‰Šé™¤\n",
        "    m = re.search(r'\\\\begin{document}(.*?)\\\\end{document}', text, re.DOTALL)\n",
        "    if m:\n",
        "        text = m.group(1)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "mRvNv8-k-I9L"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re, glob\n",
        "\n",
        "all_texts = []\n",
        "for fname in glob.glob(\"papers/**/*.tex\", recursive=True):\n",
        "    try:\n",
        "        with open(fname, encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            raw = f.read()\n",
        "            cleaned = extract_latex_body(raw)  # â† ã‚ãªãŸã®å®šç¾©ã—ãŸé–¢æ•°\n",
        "            if len(cleaned) > 200:\n",
        "                all_texts.append(cleaned)\n",
        "                # print(f\"æŠ½å‡ºæˆåŠŸ: {fname}\")\n",
        "    except Exception as e:\n",
        "        print(f\"{fname}: {e}\")\n",
        "\n",
        "print(\"æŠ½å‡ºã•ã‚ŒãŸè«–æ–‡æœ¬æ•°:\", len(all_texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c275-ax-9bzn",
        "outputId": "fcb1c843-7f4a-4a8a-cb71-e67422916bc1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "æŠ½å‡ºã•ã‚ŒãŸè«–æ–‡æœ¬æ•°: 817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# all_texts ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½œã‚‹ã¨ã“ã‚ï¼ˆâ†ã“ã‚ŒãŒå…ˆï¼‰\n",
        "\n",
        "TOKEN_PATTERN = r'\\\\[a-zA-Z]+|[{}_^=+\\-\\*/()0-9a-zA-Z]+'\n",
        "SPECIAL_TOKENS = [\"<unk>\", \"<bos>\", \"<eos>\"]\n",
        "\n",
        "tokens = []\n",
        "for txt in all_texts:\n",
        "    # å„è«–æ–‡ã”ã¨ã« <bos>, <eos> ã‚’æŒŸã‚“ã§ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º\n",
        "    body_tokens = re.findall(TOKEN_PATTERN, txt)\n",
        "    tokens.extend([\"<bos>\"] + body_tokens + [\"<eos>\"])\n",
        "\n",
        "print(f\"ãƒˆãƒ¼ã‚¯ãƒ³ç·æ•°: {len(tokens):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLxnu0p7Myb5",
        "outputId": "8b3adea5-f385-48bd-d7fb-0f8d4897f740"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ãƒˆãƒ¼ã‚¯ãƒ³ç·æ•°: 1,986,646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_latex_body(tex_text: str) -> str:\n",
        "    \"\"\"LaTeXæœ¬æ–‡ã‚’æŠ½å‡ºã—ã¦ã‚ã‚‹ç¨‹åº¦ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹ï¼ˆç°¡æ˜“ç‰ˆï¼‰\"\"\"\n",
        "    # ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã‚’å‰Šé™¤ï¼ˆ% ä»¥é™ï¼‰\n",
        "    text = re.sub(r'%.*', '', tex_text)\n",
        "\n",
        "    # \\begin{document}ã€œ\\end{document} ã®é–“ã ã‘ã‚’æŠœãå‡ºã—\n",
        "    m = re.search(r'\\\\begin{document}(.*?)\\\\end{document}', text, re.DOTALL)\n",
        "    if m:\n",
        "        text = m.group(1)\n",
        "\n",
        "    # bibliography / å‚è€ƒæ–‡çŒ®ãƒ»ä»˜éŒ²ä»¥é™ã‚’ã–ã£ãã‚Šå‰Šã‚‹ï¼ˆå¿…è¦ã«å¿œã˜ã¦èª¿æ•´ï¼‰\n",
        "    text = re.split(r'\\\\bibliography|\\\\begin{thebibliography}', text)[0]\n",
        "    text = re.split(r'\\\\appendix', text)[0]\n",
        "\n",
        "    # é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«ã¾ã¨ã‚ã‚‹\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "all_texts = []\n",
        "for fname in glob.glob(\"papers/**/*.tex\", recursive=True):\n",
        "    try:\n",
        "        with open(fname, encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            raw = f.read()\n",
        "        cleaned = extract_latex_body(raw)\n",
        "        if len(cleaned) > 200:  # ã”ãçŸ­ã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯ãƒã‚¤ã‚ºãªã®ã§é™¤å¤–\n",
        "            all_texts.append(cleaned)\n",
        "            #print(f\"xtracted from: {fname}\")\n",
        "    except Exception as e:\n",
        "        print(f\"{fname}: {e}\")\n",
        "\n",
        "print(f\"\\næŠ½å‡ºã•ã‚ŒãŸè«–æ–‡æœ¬æ•°: {len(all_texts)}\")\n",
        "print(f\"ç·ãƒ†ã‚­ã‚¹ãƒˆé•·: {sum(len(t) for t in all_texts):,} æ–‡å­—\")\n",
        "\n",
        "# ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸€ã¤ã«ã¾ã¨ã‚ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ä¿å­˜ã—ã¦ãŠãï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰\n",
        "merged_text = \"\\n\\n\".join(all_texts)\n",
        "with open(\"merged_corpus.tex\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(merged_text)\n",
        "print(\"\\nmerged_corpus.tex ã«ä¿å­˜ã—ã¾ã—ãŸ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dj8HIubk2O2K",
        "outputId": "a7457395-9ba4-470f-883c-a8afb92110e8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "æŠ½å‡ºã•ã‚ŒãŸè«–æ–‡æœ¬æ•°: 796\n",
            "ç·ãƒ†ã‚­ã‚¹ãƒˆé•·: 10,881,347 æ–‡å­—\n",
            "\n",
            "merged_corpus.tex ã«ä¿å­˜ã—ã¾ã—ãŸ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¨èªå½™è¾æ›¸ä½œæˆ"
      ],
      "metadata": {
        "id": "Hfxny7vSoFQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# SPECIAL_TOKENS ã¯ã•ã£ãã¨åŒã˜ã‚‚ã®ã‚’ä½¿ã†\n",
        "# SPECIAL_TOKENS = [\"<unk>\", \"<bos>\", \"<eos>\"]\n",
        "\n",
        "print(f\"ãƒˆãƒ¼ã‚¯ãƒ³ç·æ•°: {len(tokens):,}\")\n",
        "\n",
        "# èªå½™ã‚’é »åº¦ä¸Šä½ã ã‘ã«åœ§ç¸®\n",
        "counter = Counter(tokens)\n",
        "\n",
        "VOCAB_LIMIT = 40000\n",
        "most_common_tokens = [\n",
        "    w for (w, c) in counter.most_common(VOCAB_LIMIT)\n",
        "    if w not in SPECIAL_TOKENS\n",
        "]\n",
        "\n",
        "vocab = SPECIAL_TOKENS + most_common_tokens\n",
        "\n",
        "word2idx = {w: i for i, w in enumerate(vocab)}\n",
        "idx2word = {i: w for i, w in enumerate(vocab)}\n",
        "\n",
        "UNK_IDX = word2idx[\"<unk>\"]\n",
        "BOS_IDX = word2idx[\"<bos>\"]\n",
        "EOS_IDX = word2idx[\"<eos>\"]\n",
        "\n",
        "print(f\"èªå½™æ•°ï¼ˆåœ§ç¸®å¾Œï¼‰: {len(vocab):,}\")\n",
        "\n",
        "# ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‚’ index ã«å¤‰æ›\n",
        "indexed_tokens = [word2idx.get(t, UNK_IDX) for t in tokens]\n",
        "\n",
        "# å¿µã®ãŸã‚ãƒã‚§ãƒƒã‚¯\n",
        "max_id = max(indexed_tokens)\n",
        "print(\"æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³ID:\", max_id)\n",
        "print(\"èªå½™ã‚µã‚¤ã‚º:\", len(vocab))\n",
        "assert max_id < len(vocab), \"ãƒˆãƒ¼ã‚¯ãƒ³IDãŒèªå½™ã‚µã‚¤ã‚ºã‚’è¶…ãˆã¦ã„ã¾ã™ï¼\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHFzQd_2y9ux",
        "outputId": "8583ac87-d456-48ec-a7c2-705c5effcbc9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ãƒˆãƒ¼ã‚¯ãƒ³ç·æ•°: 1,986,646\n",
            "èªå½™æ•°ï¼ˆåœ§ç¸®å¾Œï¼‰: 40,001\n",
            "æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³ID: 40000\n",
            "èªå½™ã‚µã‚¤ã‚º: 40001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®šç¾©"
      ],
      "metadata": {
        "id": "SSNzsdoh7p20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Datasetã‚¯ãƒ©ã‚¹å®šç¾©"
      ],
      "metadata": {
        "id": "3EfH4t9CoW1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LatexDataset(Dataset):\n",
        "    \"\"\"å¤§ããªãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‹ã‚‰ã€ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒ«ã‚’åˆ‡ã‚Šå‡ºã™ Dataset\"\"\"\n",
        "    def __init__(self, data, seq_length=60, samples_per_epoch=3000):\n",
        "        self.data = data                      # â† ã™ã§ã« index åŒ–ã•ã‚ŒãŸåˆ—\n",
        "        self.seq_length = seq_length\n",
        "        self.samples_per_epoch = samples_per_epoch\n",
        "\n",
        "        self.max_start = len(self.data) - (self.seq_length + 1)\n",
        "        assert self.max_start > 0, \"ãƒ‡ãƒ¼ã‚¿ãŒçŸ­ã™ãã¾ã™\"\n",
        "\n",
        "    def __len__(self):\n",
        "        # 1epochã‚ãŸã‚Šã®ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’ç›´æ¥æ±ºã‚ã‚‹\n",
        "        return self.samples_per_epoch\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # idx ã¯ä½¿ã‚ãšã€æ¯å›ãƒ©ãƒ³ãƒ€ãƒ ãªä½ç½®ã‹ã‚‰åˆ‡ã‚Šå‡ºã™\n",
        "        start = random.randint(0, self.max_start)\n",
        "        end = start + self.seq_length + 1\n",
        "        chunk = self.data[start:end]\n",
        "\n",
        "        x = torch.tensor(chunk[:-1], dtype=torch.long)  # [T]\n",
        "        y = torch.tensor(chunk[1:], dtype=torch.long)   # [T]\n",
        "        return x, y\n"
      ],
      "metadata": {
        "id": "QEQH22RszXxJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ã“ã“ã§åˆã‚ã¦ dataset ã‚’ä½œã‚‹ï¼ˆtokens ã§ã¯ãªã indexed_tokens ã‚’æ¸¡ã™ï¼‰\n",
        "seq_length = 128\n",
        "samples_per_epoch = 8000\n",
        "dataset = LatexDataset(indexed_tokens, seq_length=seq_length,\n",
        "                       samples_per_epoch=samples_per_epoch)\n"
      ],
      "metadata": {
        "id": "yQCAnpvHHnUu"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train / Validation ã«åˆ†å‰²ã—ã¦ DataLoader ã‚’ä½œæˆ"
      ],
      "metadata": {
        "id": "I9g7MOlx7G6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio = 0.9\n",
        "n_total = len(dataset)\n",
        "n_train = int(n_total * train_ratio)\n",
        "n_val = n_total - n_train\n",
        "\n",
        "train_ds, val_ds = random_split(dataset, [n_train, n_val])\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"ãƒ‡ãƒ¼ã‚¿æ•°: total={n_total}, train={len(train_ds)}, val={len(val_ds)}\")\n",
        "print(f\"1epoch ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆtrainï¼‰: {len(train_loader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR0X-amZ64rH",
        "outputId": "478a30be-ea4b-45c8-cd92-c1345b856093"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ãƒ‡ãƒ¼ã‚¿æ•°: total=8000, train=7200, val=800\n",
            "1epoch ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆtrainï¼‰: 225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### å­¦ç¿’"
      ],
      "metadata": {
        "id": "eHHlGpd272Ve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Transformerãƒ¢ãƒ‡ãƒ«å®šç¾©(ç–‘ä¼¼æ•°å­¦è«–æ–‡ç”Ÿæˆ)"
      ],
      "metadata": {
        "id": "6WpX1NvGqTO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformerãƒ¢ãƒ‡ãƒ«å®šç¾©(ç–‘ä¼¼æ•°å­¦è«–æ–‡ç”Ÿæˆ)\n",
        "class LatexTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    LaTeX ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ç”¨ã®ã‚·ãƒ³ãƒ—ãƒ«ãª Transformer è¨€èªãƒ¢ãƒ‡ãƒ«\n",
        "    å…¥åŠ›: x [B, T]  (ãƒˆãƒ¼ã‚¯ãƒ³ID)\n",
        "    å‡ºåŠ›: logits [B, T, V]  (å„ä½ç½®ã”ã¨ã®å˜èªåˆ†å¸ƒã®ãƒ­ã‚¸ãƒƒãƒˆ)\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 vocab_size: int,\n",
        "                 d_model: int = 384,\n",
        "                 nhead: int = 6,\n",
        "                 num_layers: int = 6,\n",
        "                 dim_feedforward: int = 1024,\n",
        "                 dropout: float = 0.1,\n",
        "                 max_seq_len: int = 512):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_len = max_seq_len\n",
        "        # èªå½™åŸ‹ã‚è¾¼ã¿\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        # ä½ç½®åŸ‹ã‚è¾¼ã¿ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãª learnable embeddingï¼‰\n",
        "        self.pos_embedding = nn.Embedding(max_seq_len, d_model)\n",
        "        # Transformer Encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True  # [B, T, E] ã§å‡¦ç†\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        # å‡ºåŠ›å±¤\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        # Causal mask ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨ã«ä¿æŒ\n",
        "        self.register_buffer(\"mask\", None, persistent=False)\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz: int, device: torch.device):\n",
        "        \"\"\"\n",
        "        è‡ªå·±å›å¸°ç”¨ã®ãƒã‚¹ã‚¯ (æœªæ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¦‹ãªã„ã‚ˆã†ã«ã™ã‚‹)\n",
        "        shape: [T, T]\n",
        "        \"\"\"\n",
        "        if (self.mask is None) or (self.mask.size(0) != sz):\n",
        "            mask = torch.full((sz, sz), float(\"-inf\"), device=device)\n",
        "            mask = torch.triu(mask, diagonal=1)\n",
        "            self.mask = mask\n",
        "        return self.mask\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: [B, T]  ãƒˆãƒ¼ã‚¯ãƒ³IDåˆ—\n",
        "        æˆ»ã‚Šå€¤:\n",
        "            logits: [B, T, V]\n",
        "        \"\"\"\n",
        "        B, T = x.size()\n",
        "        device = x.device\n",
        "\n",
        "        # ä½ç½®ID [0, 1, ..., T-1]\n",
        "        positions = torch.arange(T, device=device).unsqueeze(0).expand(B, T)  # [B, T]\n",
        "\n",
        "        tok_emb = self.token_embedding(x)          # [B, T, d_model]\n",
        "        pos_emb = self.pos_embedding(positions)    # [B, T, d_model]\n",
        "        h = tok_emb + pos_emb                      # [B, T, d_model]\n",
        "\n",
        "        # causal mask (æœªæ¥ã®æƒ…å ±ã‚’è¦‹ãªã„)\n",
        "        src_mask = self._generate_square_subsequent_mask(T, device=device)  # [T, T]\n",
        "\n",
        "        # TransformerEncoder ã«é€šã™\n",
        "        # batch_first=True ãªã®ã§ h: [B, T, d_model], mask ã¯ [T, T]\n",
        "        h = self.transformer(h, mask=src_mask)     # [B, T, d_model]\n",
        "\n",
        "        logits = self.fc_out(h)                    # [B, T, V]\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "h7m2BKKEobY6"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ç¹°ã‚Šè¿”ã—è¨ˆç®—"
      ],
      "metadata": {
        "id": "NKqEGskKqeEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"device: {device}\")\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "model = LatexTransformer(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=384,\n",
        "    nhead=6,\n",
        "    num_layers=6,\n",
        "    dim_feedforward=1024,\n",
        "    dropout=0.1,\n",
        "    max_seq_len=512       # seq_length ã‚ˆã‚Šååˆ†å¤§ãã‘ã‚Œã°OK\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "num_epochs = 30\n",
        "max_grad_norm = 1.0  # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã®æœ€å¤§ãƒãƒ«ãƒ \n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # -------------------------------\n",
        "    # Train\n",
        "    # -------------------------------\n",
        "    model.train()\n",
        "    total_train_loss = 0.0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        # x, y: [B, T]\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)  # [B, T, V]\n",
        "\n",
        "        # CrossEntropyLoss ã¯ [N, C] vs [N] ã‚’ã¨ã‚‹ã®ã§ reshape ã™ã‚‹\n",
        "        loss = criterion(\n",
        "            logits.view(-1, logits.size(-1)),  # [B*T, V]\n",
        "            y.view(-1)                         # [B*T]\n",
        "        )\n",
        "        loss.backward()\n",
        "\n",
        "        # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆå‹¾é…çˆ†ç™ºå¯¾ç­–ï¼‰\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Validation\n",
        "    # -------------------------------\n",
        "    model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            val_loss = criterion(\n",
        "                logits.view(-1, logits.size(-1)),\n",
        "                y.view(-1)\n",
        "            )\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "    # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’é€²ã‚ã‚‹\n",
        "    scheduler.step()\n",
        "\n",
        "    print(\n",
        "        f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "        f\"train_loss={avg_train_loss:.4f}  val_loss={avg_val_loss:.4f}  \"\n",
        "        f\"lr={scheduler.get_last_lr()[0]:.5f}\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMzBAuxWqcea",
        "outputId": "38a9ab11-eb34-4196-975c-b0fbe584497f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n",
            "Epoch [1/30] train_loss=7.9218  val_loss=7.7958  lr=0.00100\n",
            "Epoch [2/30] train_loss=7.7953  val_loss=7.7725  lr=0.00100\n",
            "Epoch [3/30] train_loss=7.7709  val_loss=7.7349  lr=0.00100\n",
            "Epoch [4/30] train_loss=7.7522  val_loss=7.7479  lr=0.00100\n",
            "Epoch [5/30] train_loss=7.7509  val_loss=7.7648  lr=0.00100\n",
            "Epoch [6/30] train_loss=7.7471  val_loss=7.7500  lr=0.00100\n",
            "Epoch [7/30] train_loss=7.7503  val_loss=7.7449  lr=0.00100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### äºˆæ¸¬"
      ],
      "metadata": {
        "id": "5H6pGlea8C3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç”¨ãƒ˜ãƒ«ãƒ‘ãƒ¼\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fDyXRUFKolPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_from_logits(logits: torch.Tensor,\n",
        "                       temperature: float = 0.8,\n",
        "                       top_k: int = 50) -> int:\n",
        "    \"\"\"\n",
        "    logits: [V] ã®1æ™‚åˆ»åˆ†ã®ãƒ­ã‚¸ãƒƒãƒˆ\n",
        "    æˆ»ã‚Šå€¤: æ¬¡ã«é¸ã¶ãƒˆãƒ¼ã‚¯ãƒ³ID\n",
        "    \"\"\"\n",
        "    # æ¸©åº¦ã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
        "    logits = logits / temperature\n",
        "\n",
        "    # top-k ã ã‘æ®‹ã—ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆæ¥µç«¯ãªãƒã‚¤ã‚ºã‚’æ¸›ã‚‰ã™ï¼‰\n",
        "    if top_k is not None and top_k > 0:\n",
        "        values, indices = torch.topk(logits, top_k)\n",
        "        probs = torch.softmax(values, dim=-1)\n",
        "        idx_in_topk = torch.multinomial(probs, 1).item()\n",
        "        next_token_id = indices[idx_in_topk].item()\n",
        "        return next_token_id\n",
        "    else:\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        next_token_id = torch.multinomial(probs, 1).item()\n",
        "        return next_token_id\n"
      ],
      "metadata": {
        "id": "Rti0zCt5owOo"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆé–¢æ•°"
      ],
      "metadata": {
        "id": "A64q9CmRqu_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(seed_text, length=200, temperature=0.8, top_k=50, seq_length=60):\n",
        "    model.eval()\n",
        "    words = re.findall(TOKEN_PATTERN, seed_text)\n",
        "\n",
        "    for _ in range(length):\n",
        "        # ğŸ”¥ ç›´è¿‘ seq_length åˆ†ã ã‘ä½¿ã†\n",
        "        seq = [word2idx.get(w, UNK_IDX) for w in words[-seq_length:]]\n",
        "\n",
        "        x = torch.tensor([seq], dtype=torch.long).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)[0, -1]  # ãƒ©ã‚¹ãƒˆæ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ã®ã¿\n",
        "\n",
        "            # <unk> ã‚’å‡ºã«ããã™ã‚‹\n",
        "            logits[UNK_IDX] -= 10.0\n",
        "\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # top-k ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆæ¨å¥¨ï¼‰\n",
        "            values, indices = torch.topk(logits, top_k)\n",
        "            probs = torch.softmax(values, dim=0)\n",
        "            next_idx = indices[torch.multinomial(probs, 1).item()].item()\n",
        "\n",
        "        words.append(idx2word[next_idx])\n",
        "\n",
        "    return \" \".join(words)\n"
      ],
      "metadata": {
        "id": "j5K4ik0Bpa3V"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚µãƒ³ãƒ—ãƒ«"
      ],
      "metadata": {
        "id": "TUKHVRGPrHT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = r\"\\section{Introduction} We consider the problem of minimizing\"\n",
        "print(\"\\nç”Ÿæˆçµæœ:\\n\")\n",
        "text = generate(seed, length=200, temperature=0.7, top_k=30, seq_length=seq_length)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcpQbqcTrAsa",
        "outputId": "28e35c01-943a-4159-d79d-42d422591e4d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ç”Ÿæˆçµæœ:\n",
            "\n",
            "\\section {Introduction} We consider the problem of minimizing a collection of a dataset of the dataset \\mathcal {D} and consisting of the dataset \\mathcal {D} R \\mathcal R \\mathcal {X} \\rightarrow \\mathcal {C} \\mathcal {D} R \\mathcal {F}) \\to \\mathcal {D} R \\mathcal {F}) \\to \\mathcal {F} \\setminus {0 } \\mathcal {F} \\to \\mathcal {C} \\mid \\mathcal {F} \\rightharpoonup \\mathcal {F}^ \\mathcal {D} } \\subseteq \\mathcal {D} } \\mathcal {C}( \\mathcal {D} } \\mathcal {F} \\setminus \\mathcal {F} ) = \\mathcal {C}( \\mathcal {D} R \\mathcal {F}) \\rightharpoonup \\mathcal {F}) ) ( \\mathcal {E}^ \\mathcal {D} R \\mathcal {F} \\mathcal {F}) \\to \\emptyset ) is said to \\mathcal {F} \\rightharpoonup \\mathbb {N}_0 ) that ( \\mathcal {C}( \\mathcal {D} R \\mathcal {F}) ) = \\mathbb {P}_{ \\geq p}( \\mathcal {D} ) \\end {definition} \\subsection {Discussion of Theorem \\ref {thm canonical_expansions}-(1) The following proposition holds \\begin {theorem} \\label {thm canonical_expansions}-(1) Let \\mathcal {D} = \\mathbb {C} be a set of data points Then the MDP \\MDP and \\Phi if there exists a set of formulas of formulas is finite for the MDP \\MDP and \\Psi = \\mathtt {T}( \\Phi \\boldsymbol \\Psi ) satisfying the state of \\MDP _ \\pi \\models \\Psi we have for any \\Phi \\Psi such that\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# LaTeX â†’ PDF å¤‰æ› (Google Colab / Linux ç’°å¢ƒç”¨)\n",
        "# =====================================================\n",
        "\n",
        "import os, subprocess\n",
        "\n",
        "# â‘  ç”Ÿæˆæ¸ˆã¿LaTeXæœ¬æ–‡ã‚’ã“ã“ã«è²¼ã‚‹ï¼ˆæ•´å½¢å¾Œã®textã‚’å…¥ã‚Œã‚‹ï¼‰\n",
        "latex_code = r\"\"\"\n",
        "\\documentclass[11pt]{article}\n",
        "\n",
        "\\usepackage{amsmath, amssymb, amsfonts}\n",
        "\\usepackage{bm}\n",
        "\\usepackage[margin=25mm]{geometry}\n",
        "\\usepackage{natbib}\n",
        "\n",
        "\\begin{document}\n",
        "\n",
        "\\begin{abstract}\n",
        "We consider the problem of minimizing a loss function over a complex\n",
        "training space. Our method aims to avoid overfitting on a single\n",
        "graph-structured dataset and to provide a unified framework that can\n",
        "handle standard and kernel-based models. We also discuss connections\n",
        "to speculative inference methods~\\citep{Li2021SpeculativeInference}\n",
        "and show how our approach can be applied across different domains.\n",
        "\\end{abstract}\n",
        "\n",
        "\\section{Introduction}\n",
        "\n",
        "We consider the problem of minimizing a learning objective over a\n",
        "training distribution while maintaining stable performance on a test\n",
        "distribution. Our model is trained across multiple domains and is\n",
        "designed to improve interpretability and robustness of the learned\n",
        "representations.\n",
        "\n",
        "In the simplest setting, let $\\theta$ denote the model parameters and\n",
        "$\\ell(\\theta)$ be the empirical loss on the training set. A generic\n",
        "gradient-based update can be written as\n",
        "\\begin{equation}\n",
        "  \\theta_{t+1}\n",
        "  = \\theta_t - \\eta \\, \\frac{\\partial \\ell(\\theta_t)}{\\partial \\theta},\n",
        "\\end{equation}\n",
        "where $\\eta > 0$ is the learning rate. Although the automatically\n",
        "generated text contained an incomplete expression, the above equation\n",
        "represents a valid example consistent with the original intent.\n",
        "\n",
        "We also assume the existence of a kernel-based representation\n",
        "$K(x, x')$ on the input space. Let $\\mathcal{R}(\\theta)$ denote a\n",
        "regularization term, and consider the regularized objective\n",
        "\\begin{equation}\n",
        "  \\mathcal{L}(\\theta)\n",
        "  = \\ell(\\theta) + \\lambda \\, \\mathcal{R}(\\theta),\n",
        "\\end{equation}\n",
        "where $\\lambda \\ge 0$ controls the strength of regularization.\n",
        "Under suitable assumptions, one can show that $\\mathcal{L}$ admits\n",
        "a finite-sample bound that depends on the complexity of the\n",
        "underlying kernel space.\n",
        "\n",
        "\\section{Conclusion}\n",
        "\n",
        "The sequence produced by the Transformer model has been rewritten\n",
        "into a compilable \\LaTeX{} document. While the original output mixed\n",
        "incomplete equations and environment mismatches, we replaced them\n",
        "with consistent mathematical expressions and a coherent structure\n",
        "consisting of an abstract, an introduction, and basic equations.\n",
        "\n",
        "\\bibliographystyle{plainnat}\n",
        "\\bibliography{references}\n",
        "\n",
        "\\end{document}\n",
        "\"\"\"\n",
        "\n",
        "# â‘¡ texãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜\n",
        "with open(\"generated.tex\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(latex_code)\n",
        "\n",
        "# â‘¢ LaTeXã‚³ãƒãƒ³ãƒ‰ãŒä½¿ãˆã‚‹ã‚ˆã†ã«TexLiveã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!apt-get update -qq\n",
        "!apt-get install -y texlive-latex-base texlive-latex-extra texlive-fonts-recommended > /dev/null\n",
        "\n",
        "# â‘£ pdflatexã§PDFã«å¤‰æ›\n",
        "!pdflatex -interaction=nonstopmode generated.tex > /dev/null\n",
        "\n",
        "# â‘¤ å‡ºåŠ›ç¢ºèª\n",
        "print(\"PDFç”Ÿæˆå®Œäº†: generated.pdf\")\n",
        "!ls -lh generated.pdf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZMXytkVrvnL",
        "outputId": "5e5f5c25-6e9a-45e1-bcb1-c0499882baf9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Extracting templates from packages: 100%\n",
            "PDFç”Ÿæˆå®Œäº†: generated.pdf\n",
            "-rw-r--r-- 1 root root 85K Nov 23 06:39 generated.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L12fyJHI-8KI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}