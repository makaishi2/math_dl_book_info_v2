{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 11.8 LSTM\n",
        "è«–æ–‡ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã•ã›ã¦ãƒŠãƒ³ãƒãƒ£ãƒƒãƒ†è«–æ–‡ã‚’latexå½¢å¼ã§å‡ºåŠ›ã•ã›ã‚‹<br>\n",
        "LSTMç‰ˆ"
      ],
      "metadata": {
        "id": "pVHUGT0PnVAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆ"
      ],
      "metadata": {
        "id": "SiCgS4G5nxHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets tqdm torch requests feedparser --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZf1dSfg2ncZ",
        "outputId": "03863642-231b-4b98-faa7-782ce770e290"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "doAWRTHmnT-E"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "import glob\n",
        "import re\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import urllib.parse\n",
        "import feedparser\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿"
      ],
      "metadata": {
        "id": "siqtNkzfn8NZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### arXiv API ã‹ã‚‰è«–æ–‡ ID ã‚’ã¾ã¨ã‚ã¦å–å¾—ã™ã‚‹é–¢æ•°"
      ],
      "metadata": {
        "id": "DmpfipBI3Gyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_arxiv_ids(category=\"cs.LG\", max_results=200):\n",
        "    \"\"\"\n",
        "    æŒ‡å®šã‚«ãƒ†ã‚´ãƒªã®æ–°ã—ã„ arXiv è«–æ–‡ ID ã‚’ã¾ã¨ã‚ã¦å–å¾—ã™ã‚‹ç°¡æ˜“é–¢æ•°\n",
        "    ä¾‹: category=\"cs.LG\"ï¼ˆMachine Learningï¼‰\n",
        "    \"\"\"\n",
        "    base_url = \"http://export.arxiv.org/api/query\"\n",
        "\n",
        "    # APIã®ã‚¯ã‚¨ãƒª: ã‚«ãƒ†ã‚´ãƒªæŒ‡å®š + æ–°ã—ã„é †\n",
        "    search_query = f\"cat:{category}\"\n",
        "    params = {\n",
        "        \"search_query\": search_query,\n",
        "        \"start\": 0,\n",
        "        \"max_results\": max_results,\n",
        "        \"sortBy\": \"submittedDate\",\n",
        "        \"sortOrder\": \"descending\",\n",
        "    }\n",
        "\n",
        "    url = base_url + \"?\" + urllib.parse.urlencode(params)\n",
        "    print(\"ğŸ” arXiv API URL:\", url)\n",
        "\n",
        "    feed = feedparser.parse(url)\n",
        "\n",
        "    paper_ids = []\n",
        "    for entry in feed.entries:\n",
        "        # entry.id ä¾‹: \"http://arxiv.org/abs/2401.12345v1\"\n",
        "        m = re.search(r'arxiv.org/abs/(\\d{4}\\.\\d+)', entry.id)\n",
        "        if m:\n",
        "            pid = m.group(1)  # \"2401.12345\"\n",
        "            paper_ids.append(pid)\n",
        "\n",
        "    print(f\"ğŸ“„ å–å¾—ã—ãŸè«–æ–‡IDæ•°: {len(paper_ids)} ä»¶\")\n",
        "    return paper_ids\n",
        "\n",
        "# ã“ã“ã§å¥½ããªã‚«ãƒ†ã‚´ãƒªãƒ»ä»¶æ•°ã‚’æŒ‡å®š\n",
        "# ä¾‹1: cs.LG (Machine Learning) æœ€æ–°ã‹ã‚‰200æœ¬\n",
        "# ä¾‹2: math.PR (Probability)\n",
        "paper_ids = fetch_arxiv_ids(category=\"cs.LG\", max_results=200)\n",
        "\n",
        "print(paper_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8z0Kiju93J5X",
        "outputId": "b19f81bb-7f8c-443d-bfc9-a44cdabfaba3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” arXiv API URL: http://export.arxiv.org/api/query?search_query=cat%3Acs.LG&start=0&max_results=200&sortBy=submittedDate&sortOrder=descending\n",
            "ğŸ“„ å–å¾—ã—ãŸè«–æ–‡IDæ•°: 200 ä»¶\n",
            "['2511.21690', '2511.21689', '2511.21686', '2511.21678', '2511.21675', '2511.21669', '2511.21668', '2511.21667', '2511.21654', '2511.21652', '2511.21638', '2511.21635', '2511.21626', '2511.21622', '2511.21613', '2511.21607', '2511.21600', '2511.21594', '2511.21590', '2511.21581', '2511.21566', '2511.21561', '2511.21560', '2511.21550', '2511.21537', '2511.21531', '2511.21526', '2511.21514', '2511.21513', '2511.21500', '2511.21490', '2511.21474', '2511.21466', '2511.21465', '2511.21437', '2511.21416', '2511.21414', '2511.21408', '2511.21397', '2511.21381', '2511.21378', '2511.21377', '2511.21369', '2511.21364', '2511.21363', '2511.21356', '2511.21354', '2511.21350', '2511.21340', '2511.21338', '2511.21335', '2511.21320', '2511.21283', '2511.21276', '2511.21257', '2511.21247', '2511.21232', '2511.21223', '2511.21215', '2511.21213', '2511.21211', '2511.21208', '2511.21181', '2511.21140', '2511.21120', '2511.21118', '2511.21115', '2511.21109', '2511.21107', '2511.21104', '2511.21103', '2511.21101', '2511.21095', '2511.21092', '2511.21089', '2511.21088', '2511.21081', '2511.21080', '2511.21076', '2511.21075', '2511.21063', '2511.21056', '2511.21054', '2511.21050', '2511.21048', '2511.21040', '2511.21038', '2511.21035', '2511.21034', '2511.21032', '2511.21019', '2511.21016', '2511.21011', '2511.21009', '2511.21008', '2511.20997', '2511.20993', '2511.20992', '2511.20991', '2511.20987', '2511.20977', '2511.20974', '2511.20963', '2511.20960', '2511.20956', '2511.20944', '2511.20941', '2511.20934', '2511.20931', '2511.20927', '2511.20922', '2511.20913', '2511.20909', '2511.20893', '2511.20889', '2511.20888', '2511.20873', '2511.20871', '2511.20870', '2511.20853', '2511.20851', '2511.20849', '2511.20848', '2511.20844', '2511.20839', '2511.20836', '2511.20834', '2511.20830', '2511.20826', '2511.20823', '2511.20821', '2511.20814', '2511.20811', '2511.20804', '2511.20799', '2511.20798', '2511.20779', '2511.20643', '2511.20641', '2511.20640', '2511.20639', '2511.20636', '2511.20629', '2511.20626', '2511.20621', '2511.20613', '2511.20612', '2511.20609', '2511.20605', '2511.20604', '2511.20601', '2511.20597', '2511.20592', '2511.20591', '2511.20587', '2511.20586', '2511.20584', '2511.20577', '2511.20570', '2511.20564', '2511.20558', '2511.20544', '2511.20543', '2511.20541', '2511.20531', '2511.20516', '2511.20509', '2511.20503', '2511.20501', '2511.20500', '2511.20490', '2511.20489', '2511.20480', '2511.20478', '2511.20474', '2511.20469', '2511.20462', '2511.20457', '2511.20456', '2511.20445', '2511.20418', '2511.20407', '2511.20406', '2511.20397', '2511.20395', '2511.20382', '2511.20380', '2511.20362', '2511.20361', '2511.20349', '2511.20347', '2511.20333', '2511.20327', '2511.20315', '2511.20293', '2511.20283', '2511.20277', '2511.20273', '2511.20258', '2511.20257']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### arXiv ã‚½ãƒ¼ã‚¹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼†å±•é–‹"
      ],
      "metadata": {
        "id": "pFum5tb93p0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"papers\", exist_ok=True)\n",
        "\n",
        "for pid in tqdm(paper_ids, desc=\"Downloading\", ncols=80, ascii=True):\n",
        "    url = f\"https://arxiv.org/src/{pid}v1\"\n",
        "    out_path = f\"papers/{pid}.tar.gz\"\n",
        "    try:\n",
        "        r = requests.get(url, timeout=10)\n",
        "        if r.status_code == 200:\n",
        "            with open(out_path, \"wb\") as f:\n",
        "                f.write(r.content)\n",
        "            # tar / tar.gz / ãã®ä»–ã®åœ§ç¸®å½¢å¼ã«æŸ”è»Ÿã«å¯¾å¿œ\n",
        "            try:\n",
        "                with tarfile.open(out_path, \"r:*\") as tar:\n",
        "                    tar.extractall(f\"papers/{pid}\")\n",
        "            except tarfile.ReadError:\n",
        "                print(f\"Skipped {pid} (not a tar file)\")\n",
        "        else:\n",
        "            print(f\"Skipped {pid} (status {r.status_code})\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {pid}: {e}\")\n"
      ],
      "metadata": {
        "id": "K9mkCl0hnX9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d961983-bd16-4832-f484-1336c004778e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:   0%|                                      | 0/200 [00:00<?, ?it/s]/tmp/ipython-input-2326405988.py:14: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(f\"papers/{pid}\")\n",
            "Downloading:  11%|###1                         | 22/200 [00:33<02:14,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.21561 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:  38%|##########8                  | 75/200 [02:04<00:56,  2.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.21089 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:  44%|############9                | 89/200 [03:27<25:21, 13.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.21034 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:  46%|#############1               | 91/200 [03:29<12:57,  7.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.21019 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:  47%|#############6               | 94/200 [03:33<05:53,  3.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.21009 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:  60%|################6           | 119/200 [04:16<01:28,  1.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.20871 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:  94%|##########################1 | 187/200 [10:27<00:20,  1.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.20382 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: 100%|############################| 200/200 [10:52<00:00,  3.26s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### çµæœç¢ºèª"
      ],
      "metadata": {
        "id": "uHy82Ndv4CZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!find papers -name \"*.tex\" | head -n 20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV_Im0bVwhlz",
        "outputId": "946d1fa5-bd8d-4352-9a75-0ebbed00698a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "papers/2511.20406/imports.tex\n",
            "papers/2511.20406/log_2025.tex\n",
            "papers/2511.21550/content/intro.tex\n",
            "papers/2511.21550/content/heavy_momentum_mamba.tex\n",
            "papers/2511.21550/content/related_work.tex\n",
            "papers/2511.21550/content/_abstract.tex\n",
            "papers/2511.21550/content/result.tex\n",
            "papers/2511.21550/content/abstract.tex\n",
            "papers/2511.21550/content/har_system.tex\n",
            "papers/2511.21550/content/vanishing_gradient.tex\n",
            "papers/2511.21550/content/intro_.tex\n",
            "papers/2511.21550/content/prelim.tex\n",
            "papers/2511.21550/content/conclusion.tex\n",
            "papers/2511.21550/content/beyond.tex\n",
            "papers/2511.21550/content/har_system_.tex\n",
            "papers/2511.21550/content/prelim_.tex\n",
            "papers/2511.21550/content/continuous_momentum.tex\n",
            "papers/2511.21550/New_IEEEtran_how-to.tex\n",
            "papers/2511.21550/bare_jrnl_new_sample4.tex\n",
            "papers/2511.21550/math_commands.tex\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ãƒ‡ãƒ¼ã‚¿åŠ å·¥"
      ],
      "metadata": {
        "id": "IO3OjIdD7bRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LaTeX æœ¬æ–‡æŠ½å‡º ï¼‹ å‰å‡¦ç†"
      ],
      "metadata": {
        "id": "OSdfMXLInWPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re, glob\n",
        "\n",
        "def extract_latex_body(tex_text: str) -> str:\n",
        "    \"\"\"LaTeXæœ¬æ–‡ã‚’æŠ½å‡ºã—ã¦ã‚ã‚‹ç¨‹åº¦ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹ï¼ˆçµ±ä¸€ç‰ˆï¼‰\"\"\"\n",
        "    # ã‚³ãƒ¡ãƒ³ãƒˆå‰Šé™¤\n",
        "    text = re.sub(r'%.*', '', tex_text)\n",
        "\n",
        "    # \\begin{document}ã€œ\\end{document} æŠœãå‡ºã—\n",
        "    m = re.search(r'\\\\begin{document}(.*?)\\\\end{document}', text, re.DOTALL)\n",
        "    if m:\n",
        "        text = m.group(1)\n",
        "\n",
        "    # bibliography / appendix ä»¥é™ã‚’ã–ã£ãã‚Šã‚«ãƒƒãƒˆ\n",
        "    text = re.split(r'\\\\bibliography|\\\\begin{thebibliography}', text)[0]\n",
        "    text = re.split(r'\\\\appendix', text)[0]\n",
        "\n",
        "    # ç©ºç™½ã‚’1ã¤ã«\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "mRvNv8-k-I9L"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_texts = []\n",
        "for fname in glob.glob(\"papers/**/*.tex\", recursive=True):\n",
        "    try:\n",
        "        with open(fname, encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            raw = f.read()\n",
        "        cleaned = extract_latex_body(raw)\n",
        "        if len(cleaned) > 200:\n",
        "            all_texts.append(cleaned)\n",
        "    except Exception as e:\n",
        "        print(f\"{fname}: {e}\")\n",
        "\n",
        "print(f\"æŠ½å‡ºã•ã‚ŒãŸè«–æ–‡æœ¬æ•°: {len(all_texts)}\")\n",
        "print(f\"ç·ãƒ†ã‚­ã‚¹ãƒˆé•·: {sum(len(t) for t in all_texts):,} æ–‡å­—\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c275-ax-9bzn",
        "outputId": "c997511b-3a2f-4c8b-ebda-9cd53a56dd4d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "æŠ½å‡ºã•ã‚ŒãŸè«–æ–‡æœ¬æ•°: 860\n",
            "ç·ãƒ†ã‚­ã‚¹ãƒˆé•·: 11,596,237 æ–‡å­—\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¨èªå½™è¾æ›¸ä½œæˆ"
      ],
      "metadata": {
        "id": "Hfxny7vSoFQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# SPECIAL_TOKENS ã¯ã•ã£ãã¨åŒã˜ã‚‚ã®ã‚’ä½¿ã†\n",
        "# SPECIAL_TOKENS = [\"<unk>\", \"<bos>\", \"<eos>\"]\n",
        "\n",
        "print(f\"ãƒˆãƒ¼ã‚¯ãƒ³ç·æ•°: {len(tokens):,}\")\n",
        "\n",
        "# èªå½™ã‚’é »åº¦ä¸Šä½ã ã‘ã«åœ§ç¸®\n",
        "counter = Counter(tokens)\n",
        "\n",
        "VOCAB_LIMIT = 40000\n",
        "most_common_tokens = [\n",
        "    w for (w, c) in counter.most_common(VOCAB_LIMIT)\n",
        "    if w not in SPECIAL_TOKENS\n",
        "]\n",
        "\n",
        "vocab = SPECIAL_TOKENS + most_common_tokens\n",
        "\n",
        "word2idx = {w: i for i, w in enumerate(vocab)}\n",
        "idx2word = {i: w for i, w in enumerate(vocab)}\n",
        "\n",
        "UNK_IDX = word2idx[\"<unk>\"]\n",
        "BOS_IDX = word2idx[\"<bos>\"]\n",
        "EOS_IDX = word2idx[\"<eos>\"]\n",
        "\n",
        "print(f\"èªå½™æ•°ï¼ˆåœ§ç¸®å¾Œï¼‰: {len(vocab):,}\")\n",
        "\n",
        "# ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‚’ index ã«å¤‰æ›\n",
        "indexed_tokens = [word2idx.get(t, UNK_IDX) for t in tokens]\n",
        "\n",
        "# å¿µã®ãŸã‚ãƒã‚§ãƒƒã‚¯\n",
        "max_id = max(indexed_tokens)\n",
        "print(\"æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³ID:\", max_id)\n",
        "print(\"èªå½™ã‚µã‚¤ã‚º:\", len(vocab))\n",
        "assert max_id < len(vocab), \"ãƒˆãƒ¼ã‚¯ãƒ³IDãŒèªå½™ã‚µã‚¤ã‚ºã‚’è¶…ãˆã¦ã„ã¾ã™ï¼\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHFzQd_2y9ux",
        "outputId": "a45d7c84-b66f-43eb-848e-95bec5b3bc94"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ãƒˆãƒ¼ã‚¯ãƒ³ç·æ•°: 2,095,436\n",
            "èªå½™æ•°ï¼ˆåœ§ç¸®å¾Œï¼‰: 40,001\n",
            "æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³ID: 40000\n",
            "èªå½™ã‚µã‚¤ã‚º: 40001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®šç¾©"
      ],
      "metadata": {
        "id": "SSNzsdoh7p20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Datasetã‚¯ãƒ©ã‚¹å®šç¾©"
      ],
      "metadata": {
        "id": "3EfH4t9CoW1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LatexDataset(Dataset):\n",
        "    \"\"\"å¤§ããªãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‹ã‚‰ã€ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒ«ã‚’åˆ‡ã‚Šå‡ºã™ Dataset\"\"\"\n",
        "    def __init__(self, data, seq_length=60, samples_per_epoch=3000):\n",
        "        self.data = data                      # â† ã™ã§ã« index åŒ–ã•ã‚ŒãŸåˆ—\n",
        "        self.seq_length = seq_length\n",
        "        self.samples_per_epoch = samples_per_epoch\n",
        "\n",
        "        self.max_start = len(self.data) - (self.seq_length + 1)\n",
        "        assert self.max_start > 0, \"ãƒ‡ãƒ¼ã‚¿ãŒçŸ­ã™ãã¾ã™\"\n",
        "\n",
        "    def __len__(self):\n",
        "        # 1epochã‚ãŸã‚Šã®ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’ç›´æ¥æ±ºã‚ã‚‹\n",
        "        return self.samples_per_epoch\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # idx ã¯ä½¿ã‚ãšã€æ¯å›ãƒ©ãƒ³ãƒ€ãƒ ãªä½ç½®ã‹ã‚‰åˆ‡ã‚Šå‡ºã™\n",
        "        start = random.randint(0, self.max_start)\n",
        "        end = start + self.seq_length + 1\n",
        "        chunk = self.data[start:end]\n",
        "\n",
        "        x = torch.tensor(chunk[:-1], dtype=torch.long)  # [T]\n",
        "        y = torch.tensor(chunk[1:], dtype=torch.long)   # [T]\n",
        "        return x, y\n"
      ],
      "metadata": {
        "id": "QEQH22RszXxJ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ã“ã“ã§åˆã‚ã¦ dataset ã‚’ä½œã‚‹ï¼ˆtokens ã§ã¯ãªã indexed_tokens ã‚’æ¸¡ã™ï¼‰\n",
        "seq_length = 128\n",
        "samples_per_epoch = 8000\n",
        "dataset = LatexDataset(indexed_tokens, seq_length=seq_length,\n",
        "                       samples_per_epoch=samples_per_epoch)\n"
      ],
      "metadata": {
        "id": "yQCAnpvHHnUu"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train / Validation ã«åˆ†å‰²ã—ã¦ DataLoader ã‚’ä½œæˆ"
      ],
      "metadata": {
        "id": "I9g7MOlx7G6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio = 0.9\n",
        "n_total = len(dataset)\n",
        "n_train = int(n_total * train_ratio)\n",
        "n_val = n_total - n_train\n",
        "\n",
        "train_ds, val_ds = random_split(dataset, [n_train, n_val])\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"ãƒ‡ãƒ¼ã‚¿æ•°: total={n_total}, train={len(train_ds)}, val={len(val_ds)}\")\n",
        "print(f\"1epoch ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆtrainï¼‰: {len(train_loader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR0X-amZ64rH",
        "outputId": "0b6b5413-d403-4dc4-93db-62104b049bbb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ãƒ‡ãƒ¼ã‚¿æ•°: total=8000, train=7200, val=800\n",
            "1epoch ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆtrainï¼‰: 225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### å­¦ç¿’"
      ],
      "metadata": {
        "id": "eHHlGpd272Ve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LSTMãƒ¢ãƒ‡ãƒ«å®šç¾©(ç–‘ä¼¼æ•°å­¦è«–æ–‡ç”Ÿæˆ)"
      ],
      "metadata": {
        "id": "6WpX1NvGqTO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTMãƒ¢ãƒ‡ãƒ«å®šç¾©ï¼ˆç–‘ä¼¼æ•°å­¦è«–æ–‡ç”Ÿæˆï¼‰\n",
        "class LatexLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    LaTeX ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ç”¨ã®ã‚·ãƒ³ãƒ—ãƒ«ãª LSTM è¨€èªãƒ¢ãƒ‡ãƒ«\n",
        "    å…¥åŠ›: x [B, T]  (ãƒˆãƒ¼ã‚¯ãƒ³ID)\n",
        "    å‡ºåŠ›: logits [B, T, V]  (å„ä½ç½®ã”ã¨ã®å˜èªåˆ†å¸ƒã®ãƒ­ã‚¸ãƒƒãƒˆ)\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 vocab_size: int,\n",
        "                 d_model: int = 384,\n",
        "                 num_layers: int = 3,\n",
        "                 dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        # èªå½™åŸ‹ã‚è¾¼ã¿\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        # LSTM\n",
        "        # batch_first=True ã«ã—ã¦ [B, T, E] å½¢å¼ã§æ‰±ãˆã‚‹ã‚ˆã†ã«ã™ã‚‹\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=d_model,\n",
        "            hidden_size=d_model,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0.0\n",
        "        )\n",
        "        # å‡ºåŠ›å±¤\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: [B, T]  ãƒˆãƒ¼ã‚¯ãƒ³IDåˆ—\n",
        "        æˆ»ã‚Šå€¤:\n",
        "            logits: [B, T, V]\n",
        "        \"\"\"\n",
        "        # åŸ‹ã‚è¾¼ã¿\n",
        "        emb = self.token_embedding(x)   # [B, T, d_model]\n",
        "        # LSTM ã«é€šã™ï¼ˆéš ã‚ŒçŠ¶æ…‹ã¯æ¯ãƒãƒƒãƒã”ã¨ã«ãƒªã‚»ãƒƒãƒˆï¼‰\n",
        "        out, _ = self.lstm(emb)        # out: [B, T, d_model]\n",
        "        # å„æ™‚åˆ»ã”ã¨ã«èªå½™åˆ†å¸ƒã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®—\n",
        "        logits = self.fc_out(out)      # [B, T, vocab_size]\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "h7m2BKKEobY6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ç¹°ã‚Šè¿”ã—è¨ˆç®—"
      ],
      "metadata": {
        "id": "NKqEGskKqeEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"device: {device}\")\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "model = LatexLSTM(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=384,      # åŸ‹ã‚è¾¼ã¿ãƒ»éš ã‚ŒçŠ¶æ…‹ã®æ¬¡å…ƒ\n",
        "    num_layers=3,     # LSTMã®å±¤æ•°ï¼ˆé©å®œèª¿æ•´å¯ï¼‰\n",
        "    dropout=0.1\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "num_epochs = 30\n",
        "max_grad_norm = 1.0  # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã®æœ€å¤§ãƒãƒ«ãƒ \n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # -------------------------------\n",
        "    # Train\n",
        "    # -------------------------------\n",
        "    model.train()\n",
        "    total_train_loss = 0.0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        # x, y: [B, T]\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)  # [B, T, V]\n",
        "\n",
        "        # CrossEntropyLoss ã¯ [N, C] vs [N] ã‚’ã¨ã‚‹ã®ã§ reshape ã™ã‚‹\n",
        "        loss = criterion(\n",
        "            logits.view(-1, logits.size(-1)),  # [B*T, V]\n",
        "            y.view(-1)                         # [B*T]\n",
        "        )\n",
        "        loss.backward()\n",
        "\n",
        "        # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆå‹¾é…çˆ†ç™ºå¯¾ç­–ï¼‰\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Validation\n",
        "    # -------------------------------\n",
        "    model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            val_loss = criterion(\n",
        "                logits.view(-1, logits.size(-1)),\n",
        "                y.view(-1)\n",
        "            )\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "    # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’é€²ã‚ã‚‹\n",
        "    scheduler.step()\n",
        "\n",
        "    print(\n",
        "        f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "        f\"train_loss={avg_train_loss:.4f}  val_loss={avg_val_loss:.4f}  \"\n",
        "        f\"lr={scheduler.get_last_lr()[0]:.5f}\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMzBAuxWqcea",
        "outputId": "37668c7f-061c-4339-9aef-f1289c662bb1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n",
            "Epoch [1/30] train_loss=8.0473  val_loss=7.8898  lr=0.00100\n",
            "Epoch [2/30] train_loss=7.8975  val_loss=7.8932  lr=0.00100\n",
            "Epoch [3/30] train_loss=7.8764  val_loss=7.8662  lr=0.00100\n",
            "Epoch [4/30] train_loss=7.8755  val_loss=7.8656  lr=0.00100\n",
            "Epoch [5/30] train_loss=7.8545  val_loss=7.8449  lr=0.00100\n",
            "Epoch [6/30] train_loss=7.8381  val_loss=7.7268  lr=0.00100\n",
            "Epoch [7/30] train_loss=7.4000  val_loss=7.0296  lr=0.00100\n",
            "Epoch [8/30] train_loss=6.6720  val_loss=6.3780  lr=0.00100\n",
            "Epoch [9/30] train_loss=6.2160  val_loss=6.0579  lr=0.00100\n",
            "Epoch [10/30] train_loss=5.9441  val_loss=5.7028  lr=0.00050\n",
            "Epoch [11/30] train_loss=5.7280  val_loss=5.6101  lr=0.00050\n",
            "Epoch [12/30] train_loss=5.6257  val_loss=5.5276  lr=0.00050\n",
            "Epoch [13/30] train_loss=5.5333  val_loss=5.3985  lr=0.00050\n",
            "Epoch [14/30] train_loss=5.4601  val_loss=5.3182  lr=0.00050\n",
            "Epoch [15/30] train_loss=5.3709  val_loss=5.2645  lr=0.00050\n",
            "Epoch [16/30] train_loss=5.2839  val_loss=5.2196  lr=0.00050\n",
            "Epoch [17/30] train_loss=5.1982  val_loss=5.1109  lr=0.00050\n",
            "Epoch [18/30] train_loss=5.1321  val_loss=5.0731  lr=0.00050\n",
            "Epoch [19/30] train_loss=5.0697  val_loss=5.0415  lr=0.00050\n",
            "Epoch [20/30] train_loss=4.9971  val_loss=4.9731  lr=0.00025\n",
            "Epoch [21/30] train_loss=4.9675  val_loss=4.8154  lr=0.00025\n",
            "Epoch [22/30] train_loss=4.8943  val_loss=4.8084  lr=0.00025\n",
            "Epoch [23/30] train_loss=4.8527  val_loss=4.8185  lr=0.00025\n",
            "Epoch [24/30] train_loss=4.8362  val_loss=4.7462  lr=0.00025\n",
            "Epoch [25/30] train_loss=4.7831  val_loss=4.7718  lr=0.00025\n",
            "Epoch [26/30] train_loss=4.7845  val_loss=4.7806  lr=0.00025\n",
            "Epoch [27/30] train_loss=4.7554  val_loss=4.7170  lr=0.00025\n",
            "Epoch [28/30] train_loss=4.7378  val_loss=4.6198  lr=0.00025\n",
            "Epoch [29/30] train_loss=4.7244  val_loss=4.6154  lr=0.00025\n",
            "Epoch [30/30] train_loss=4.6942  val_loss=4.6421  lr=0.00013\n",
            "CPU times: user 21min 1s, sys: 6.42 s, total: 21min 8s\n",
            "Wall time: 21min 18s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### äºˆæ¸¬"
      ],
      "metadata": {
        "id": "5H6pGlea8C3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç”¨ãƒ˜ãƒ«ãƒ‘ãƒ¼\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fDyXRUFKolPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_from_logits(logits: torch.Tensor,\n",
        "                       temperature: float = 0.8,\n",
        "                       top_k: int = 50) -> int:\n",
        "    \"\"\"\n",
        "    logits: [V] ã®1æ™‚åˆ»åˆ†ã®ãƒ­ã‚¸ãƒƒãƒˆ\n",
        "    æˆ»ã‚Šå€¤: æ¬¡ã«é¸ã¶ãƒˆãƒ¼ã‚¯ãƒ³ID\n",
        "    \"\"\"\n",
        "    # æ¸©åº¦ã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
        "    logits = logits / temperature\n",
        "\n",
        "    # top-k ã ã‘æ®‹ã—ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆæ¥µç«¯ãªãƒã‚¤ã‚ºã‚’æ¸›ã‚‰ã™ï¼‰\n",
        "    if top_k is not None and top_k > 0:\n",
        "        values, indices = torch.topk(logits, top_k)\n",
        "        probs = torch.softmax(values, dim=-1)\n",
        "        idx_in_topk = torch.multinomial(probs, 1).item()\n",
        "        next_token_id = indices[idx_in_topk].item()\n",
        "        return next_token_id\n",
        "    else:\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        next_token_id = torch.multinomial(probs, 1).item()\n",
        "        return next_token_id\n"
      ],
      "metadata": {
        "id": "Rti0zCt5owOo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆé–¢æ•°"
      ],
      "metadata": {
        "id": "A64q9CmRqu_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(seed_text, length=200, temperature=0.8, top_k=50, seq_length=60):\n",
        "    model.eval()\n",
        "    words = re.findall(TOKEN_PATTERN, seed_text)\n",
        "\n",
        "    for _ in range(length):\n",
        "        # ğŸ”¥ ç›´è¿‘ seq_length åˆ†ã ã‘ä½¿ã†\n",
        "        seq = [word2idx.get(w, UNK_IDX) for w in words[-seq_length:]]\n",
        "\n",
        "        x = torch.tensor([seq], dtype=torch.long).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)[0, -1]  # ãƒ©ã‚¹ãƒˆæ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ã®ã¿\n",
        "\n",
        "            # <unk> ã‚’å‡ºã«ããã™ã‚‹\n",
        "            logits[UNK_IDX] -= 10.0\n",
        "\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # top-k ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆæ¨å¥¨ï¼‰\n",
        "            values, indices = torch.topk(logits, top_k)\n",
        "            probs = torch.softmax(values, dim=0)\n",
        "            next_idx = indices[torch.multinomial(probs, 1).item()].item()\n",
        "\n",
        "        words.append(idx2word[next_idx])\n",
        "\n",
        "    return \" \".join(words)\n"
      ],
      "metadata": {
        "id": "j5K4ik0Bpa3V"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚µãƒ³ãƒ—ãƒ«"
      ],
      "metadata": {
        "id": "TUKHVRGPrHT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = r\"\"\"\n",
        "\\section{Mathematical Formulation}\n",
        "We define the objective function as\n",
        "\\begin{equation}\n",
        "\"\"\"\n",
        "\n",
        "text = generate(seed, length=200, temperature=0.6, top_k=25, seq_length=seq_length)\n",
        "print(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avboJFZsc4Jl",
        "outputId": "41d8e9c4-7f67-42c3-922d-5937437e0a05"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\section {Mathematical Formulation} We define the objective function as \\begin {equation} \\mathcal {L}_{ \\text {SFT}} ( \\theta }(s \\mathcal {L}_{ \\text {SFT}}( \\theta ^{ \\theta x^i y^i) \\end {equation} where \\mathcal {V} is the number of nodes is the set of a hidden distribution of the loss of the state space This is not a single estimate of the distribution of the input and the predicted embedding is then a small number of values in the first stage \\begin {equation} \\label {eq graph_obj} \\mathbb {E} \\left ( \\mathbf {X}^ \\top } \\mathrm {cal}} \\right ) \\right ) = \\hat { \\mathbf {x}} + \\mathbf {p} = \\mathbf {x}) - \\mathbf {W}_{ \\mathrm {aug}}^{( \\top \\mathbf {I}) \\end {equation} where \\boldsymbol { \\mathbf {x}} = \\mathbf {b} \\cdot \\mathbf {x} ) We obtain the following set of the number of concepts in the current model we can have \\begin {align} \\mathcal {L}( \\theta ) = \\frac { \\mathcal {L}_{ \\text {in} \\phi ( \\theta _ \\text {d}^2 \\sigma _{ \\text {dq} \\W _ \\text {rope} \\mathbf {R}^ \\mathbf {x}_{1 N}^0} \\hat { \\mathbf {x}} + \\mathbf {1} ( \\mathbf {s} \\mathbf {a}) - \\mathbf { \\mathbf {x}} \\quad \\quad \\end {equation} The model is trained with the loss function \\begin {equation} \\mathcal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### æ•´å½¢\n",
        "ä¸Šã®çµæœã¯ç”ŸæˆAIã§æ•´å½¢ã‚’ã‹ã‘ã¾ã™ã€‚ãã®çµæœã‚’ä¸‹ã®notebookã®r\"\"\"ã‹ã‚‰\"\"\"ã®é–“ã«è²¼ã‚Šä»˜ã‘ã¾ã™"
      ],
      "metadata": {
        "id": "B2mysJHOdeGg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PDFç”Ÿæˆ"
      ],
      "metadata": {
        "id": "zmSd14BUeBoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# LaTeX â†’ PDF å¤‰æ› (Google Colab / Linux ç’°å¢ƒç”¨)\n",
        "# =====================================================\n",
        "\n",
        "import os, subprocess\n",
        "\n",
        "# â‘  ç”Ÿæˆæ¸ˆã¿LaTeXæœ¬æ–‡ã‚’ã“ã“ã«è²¼ã‚‹ï¼ˆæ•´å½¢å¾Œã®textã‚’å…¥ã‚Œã‚‹ï¼‰\n",
        "latex_code = r\"\"\"\n",
        "\\documentclass{article}\n",
        "\\usepackage[T1]{fontenc}\n",
        "\\usepackage{lmodern}\n",
        "\\usepackage{amsmath, amssymb}\n",
        "\\usepackage{bm}\n",
        "\n",
        "\\begin{document}\n",
        "\n",
        "\\section{Mathematical Formulation}\n",
        "\n",
        "We define the overall training objective in terms of a supervised fine-tuning (SFT) loss over the model parameters $\\theta$. Given an input token sequence $\\bm{x}^i$ and the corresponding target $\\bm{y}^i$, the loss function can be written as\n",
        "\n",
        "\\begin{equation}\n",
        "    \\mathcal{L}_{\\mathrm{SFT}}(\\theta)\n",
        "    =\n",
        "    - \\sum_{i=1}^{N}\n",
        "    \\log p_{\\theta}(\\bm{y}^i \\mid \\bm{x}^i),\n",
        "    \\label{eq:sft}\n",
        "\\end{equation}\n",
        "where $N$ is the number of training samples, and $p_{\\theta}$ denotes the conditional probability defined by the model.\n",
        "\n",
        "In order to incorporate latent structure, we consider an embedding $\\bm{z}$ derived from $\\bm{x}$, and reformulate the objective as\n",
        "\n",
        "\\begin{equation}\n",
        "    \\mathcal{L}(\\theta)\n",
        "    =\n",
        "    \\mathbb{E}_{\\bm{x}}\n",
        "    \\left[\n",
        "        - \\sum_{t=1}^{T}\n",
        "        \\log p_{\\theta}(x_t \\mid x_{<t}, \\bm{z})\n",
        "    \\right]\n",
        "    +\n",
        "    \\lambda \\, \\|\\theta\\|^2_2,\n",
        "    \\label{eq:latent_obj}\n",
        "\\end{equation}\n",
        "where $x_{<t}$ denotes the preceding token sequence, $\\lambda$ is a regularization coefficient, and $\\|\\theta\\|_2$ is the $\\ell_2$ norm of model parameters.\n",
        "\n",
        "To integrate graph-based relational information, we define an auxiliary objective\n",
        "\n",
        "\\begin{equation}\n",
        "    \\mathcal{L}_{\\mathrm{graph}}\n",
        "    =\n",
        "    \\left\\|\n",
        "        \\bm{W}_{\\mathrm{aug}}^{\\top} \\bm{x}\n",
        "        -\n",
        "        \\hat{\\bm{x}}\n",
        "    \\right\\|_2^2,\n",
        "    \\label{eq:graph}\n",
        "\\end{equation}\n",
        "where $\\bm{W}_{\\mathrm{aug}}$ is a learnable transformation matrix and $\\hat{\\bm{x}}$ is a predicted latent reconstruction.\n",
        "\n",
        "Combining (\\ref{eq:sft}) and (\\ref{eq:graph}), our final objective becomes\n",
        "\n",
        "\\begin{equation}\n",
        "    \\mathcal{L}_{\\mathrm{total}}(\\theta)\n",
        "    =\n",
        "    \\mathcal{L}_{\\mathrm{SFT}}(\\theta)\n",
        "    +\n",
        "    \\alpha \\, \\mathcal{L}_{\\mathrm{graph}},\n",
        "    \\label{eq:total}\n",
        "\\end{equation}\n",
        "where $\\alpha$ controls the contribution of the structural regularization term.\n",
        "\n",
        "The model parameters are updated via gradient descent as\n",
        "\n",
        "\\begin{equation}\n",
        "    \\theta_{t+1}\n",
        "    =\n",
        "    \\theta_t\n",
        "    -\n",
        "    \\eta \\,\n",
        "    \\nabla_{\\theta} \\mathcal{L}_{\\mathrm{total}}(\\theta_t),\n",
        "\\end{equation}\n",
        "where $\\eta$ denotes the learning rate.\n",
        "\n",
        "\\end{document}\n",
        "\"\"\"\n",
        "\n",
        "# â‘¡ texãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜\n",
        "with open(\"generated.tex\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(latex_code)\n",
        "\n",
        "# â‘¢ LaTeXã‚³ãƒãƒ³ãƒ‰ãŒä½¿ãˆã‚‹ã‚ˆã†ã«TexLiveã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!apt-get update -qq\n",
        "!apt-get install -y texlive-latex-base texlive-latex-extra texlive-fonts-recommended > /dev/null\n",
        "\n",
        "# â‘£ pdflatexã§PDFã«å¤‰æ›\n",
        "!pdflatex -interaction=nonstopmode generated.tex > /dev/null\n",
        "\n",
        "# â‘¤ å‡ºåŠ›ç¢ºèª\n",
        "print(\"PDFç”Ÿæˆå®Œäº†: generated.pdf\")\n",
        "!ls -lh generated.pdf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZMXytkVrvnL",
        "outputId": "c8188b57-df99-4fe9-acc0-1bf5c532f005"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "PDFç”Ÿæˆå®Œäº†: generated.pdf\n",
            "-rw-r--r-- 1 root root 135K Nov 29 11:52 generated.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L12fyJHI-8KI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}