{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 11.8 RNN/LSTM\n",
        "è«–æ–‡ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã•ã›ã¦ãƒŠãƒ³ãƒãƒ£ãƒƒãƒ†è«–æ–‡ã‚’latexå½¢å¼ã§å‡ºåŠ›ã•ã›ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’LSTMã§ä½œã‚‹"
      ],
      "metadata": {
        "id": "pVHUGT0PnVAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆ"
      ],
      "metadata": {
        "id": "SiCgS4G5nxHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets tqdm torch requests feedparser --quiet"
      ],
      "metadata": {
        "id": "uZf1dSfg2ncZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doAWRTHmnT-E"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "import glob\n",
        "import re\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import urllib.parse\n",
        "import feedparser\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿"
      ],
      "metadata": {
        "id": "siqtNkzfn8NZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### arXiv API ã‹ã‚‰è«–æ–‡ ID ã‚’ã¾ã¨ã‚ã¦å–å¾—ã™ã‚‹é–¢æ•°"
      ],
      "metadata": {
        "id": "DmpfipBI3Gyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_arxiv_ids(category=\"cs.LG\", max_results=200):\n",
        "    \"\"\"\n",
        "    æŒ‡å®šã‚«ãƒ†ã‚´ãƒªã®æ–°ã—ã„ arXiv è«–æ–‡ ID ã‚’ã¾ã¨ã‚ã¦å–å¾—ã™ã‚‹ç°¡æ˜“é–¢æ•°\n",
        "    ä¾‹: category=\"cs.LG\"ï¼ˆMachine Learningï¼‰\n",
        "    \"\"\"\n",
        "    base_url = \"http://export.arxiv.org/api/query\"\n",
        "\n",
        "    # APIã®ã‚¯ã‚¨ãƒª: ã‚«ãƒ†ã‚´ãƒªæŒ‡å®š + æ–°ã—ã„é †\n",
        "    search_query = f\"cat:{category}\"\n",
        "    params = {\n",
        "        \"search_query\": search_query,\n",
        "        \"start\": 0,\n",
        "        \"max_results\": max_results,\n",
        "        \"sortBy\": \"submittedDate\",\n",
        "        \"sortOrder\": \"descending\",\n",
        "    }\n",
        "\n",
        "    url = base_url + \"?\" + urllib.parse.urlencode(params)\n",
        "    print(\"ğŸ” arXiv API URL:\", url)\n",
        "\n",
        "    feed = feedparser.parse(url)\n",
        "\n",
        "    paper_ids = []\n",
        "    for entry in feed.entries:\n",
        "        # entry.id ä¾‹: \"http://arxiv.org/abs/2401.12345v1\"\n",
        "        m = re.search(r'arxiv.org/abs/(\\d{4}\\.\\d+)', entry.id)\n",
        "        if m:\n",
        "            pid = m.group(1)  # \"2401.12345\"\n",
        "            paper_ids.append(pid)\n",
        "\n",
        "    print(f\"ğŸ“„ å–å¾—ã—ãŸè«–æ–‡IDæ•°: {len(paper_ids)} ä»¶\")\n",
        "    return paper_ids\n",
        "\n",
        "# ã“ã“ã§å¥½ããªã‚«ãƒ†ã‚´ãƒªãƒ»ä»¶æ•°ã‚’æŒ‡å®š\n",
        "# ä¾‹1: cs.LG (Machine Learning) æœ€æ–°ã‹ã‚‰200æœ¬\n",
        "# ä¾‹2: math.PR (Probability)\n",
        "paper_ids = fetch_arxiv_ids(category=\"cs.LG\", max_results=200)\n",
        "\n",
        "print(paper_ids)"
      ],
      "metadata": {
        "id": "8z0Kiju93J5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### arXiv ã‚½ãƒ¼ã‚¹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼†å±•é–‹"
      ],
      "metadata": {
        "id": "pFum5tb93p0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"papers\", exist_ok=True)\n",
        "\n",
        "for pid in tqdm(paper_ids, desc=\"Downloading\", ncols=80, ascii=True):\n",
        "    url = f\"https://arxiv.org/src/{pid}v1\"\n",
        "    out_path = f\"papers/{pid}.tar.gz\"\n",
        "    try:\n",
        "        r = requests.get(url, timeout=10)\n",
        "        if r.status_code == 200:\n",
        "            with open(out_path, \"wb\") as f:\n",
        "                f.write(r.content)\n",
        "            # tar / tar.gz / ãã®ä»–ã®åœ§ç¸®å½¢å¼ã«æŸ”è»Ÿã«å¯¾å¿œ\n",
        "            try:\n",
        "                with tarfile.open(out_path, \"r:*\") as tar:\n",
        "                    tar.extractall(f\"papers/{pid}\")\n",
        "            except tarfile.ReadError:\n",
        "                print(f\"Skipped {pid} (not a tar file)\")\n",
        "        else:\n",
        "            print(f\"Skipped {pid} (status {r.status_code})\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {pid}: {e}\")\n"
      ],
      "metadata": {
        "id": "K9mkCl0hnX9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### çµæœç¢ºèª"
      ],
      "metadata": {
        "id": "uHy82Ndv4CZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!find papers -name \"*.tex\" | head -n 20"
      ],
      "metadata": {
        "id": "xV_Im0bVwhlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ãƒ‡ãƒ¼ã‚¿åŠ å·¥"
      ],
      "metadata": {
        "id": "IO3OjIdD7bRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LaTeX æœ¬æ–‡æŠ½å‡º ï¼‹ å‰å‡¦ç†"
      ],
      "metadata": {
        "id": "OSdfMXLInWPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_latex_body(tex_text):\n",
        "    \"\"\"LaTeXæœ¬æ–‡ã‚’æŠ½å‡ºã—ã¦ã‚¯ãƒªãƒ¼ãƒ³åŒ–\"\"\"\n",
        "    text = re.sub(r'%.*', '', tex_text)  # ã‚³ãƒ¡ãƒ³ãƒˆå‰Šé™¤\n",
        "    m = re.search(r'\\\\begin{document}(.*?)\\\\end{document}', text, re.DOTALL)\n",
        "    if m:\n",
        "        text = m.group(1)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "mRvNv8-k-I9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re, glob\n",
        "\n",
        "all_texts = []\n",
        "for fname in glob.glob(\"papers/**/*.tex\", recursive=True):\n",
        "    try:\n",
        "        with open(fname, encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            raw = f.read()\n",
        "            cleaned = extract_latex_body(raw)  # â† ã‚ãªãŸã®å®šç¾©ã—ãŸé–¢æ•°\n",
        "            if len(cleaned) > 200:\n",
        "                all_texts.append(cleaned)\n",
        "                # print(f\"æŠ½å‡ºæˆåŠŸ: {fname}\")\n",
        "    except Exception as e:\n",
        "        print(f\"{fname}: {e}\")\n",
        "\n",
        "print(\"æŠ½å‡ºã•ã‚ŒãŸè«–æ–‡æœ¬æ•°:\", len(all_texts))"
      ],
      "metadata": {
        "id": "c275-ax-9bzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all_texts ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½œã‚‹ã¨ã“ã‚ï¼ˆâ†ã“ã‚ŒãŒå…ˆï¼‰\n",
        "\n",
        "TOKEN_PATTERN = r'\\\\[a-zA-Z]+|[{}_^=+\\-\\*/()0-9a-zA-Z]+'\n",
        "SPECIAL_TOKENS = [\"<unk>\", \"<bos>\", \"<eos>\"]\n",
        "\n",
        "tokens = []\n",
        "for txt in all_texts:\n",
        "    # å„è«–æ–‡ã”ã¨ã« <bos>, <eos> ã‚’æŒŸã‚“ã§ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º\n",
        "    body_tokens = re.findall(TOKEN_PATTERN, txt)\n",
        "    tokens.extend([\"<bos>\"] + body_tokens + [\"<eos>\"])\n",
        "\n",
        "print(f\"ãƒˆãƒ¼ã‚¯ãƒ³ç·æ•°: {len(tokens):,}\")"
      ],
      "metadata": {
        "id": "cLxnu0p7Myb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_latex_body(tex_text: str) -> str:\n",
        "    \"\"\"LaTeXæœ¬æ–‡ã‚’æŠ½å‡ºã—ã¦ã‚ã‚‹ç¨‹åº¦ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹ï¼ˆç°¡æ˜“ç‰ˆï¼‰\"\"\"\n",
        "    # ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã‚’å‰Šé™¤ï¼ˆ% ä»¥é™ï¼‰\n",
        "    text = re.sub(r'%.*', '', tex_text)\n",
        "\n",
        "    # \\begin{document}ã€œ\\end{document} ã®é–“ã ã‘ã‚’æŠœãå‡ºã—\n",
        "    m = re.search(r'\\\\begin{document}(.*?)\\\\end{document}', text, re.DOTALL)\n",
        "    if m:\n",
        "        text = m.group(1)\n",
        "\n",
        "    # bibliography / å‚è€ƒæ–‡çŒ®ãƒ»ä»˜éŒ²ä»¥é™ã‚’ã–ã£ãã‚Šå‰Šã‚‹ï¼ˆå¿…è¦ã«å¿œã˜ã¦èª¿æ•´ï¼‰\n",
        "    text = re.split(r'\\\\bibliography|\\\\begin{thebibliography}', text)[0]\n",
        "    text = re.split(r'\\\\appendix', text)[0]\n",
        "\n",
        "    # é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«ã¾ã¨ã‚ã‚‹\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "all_texts = []\n",
        "for fname in glob.glob(\"papers/**/*.tex\", recursive=True):\n",
        "    try:\n",
        "        with open(fname, encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            raw = f.read()\n",
        "        cleaned = extract_latex_body(raw)\n",
        "        if len(cleaned) > 200:  # ã”ãçŸ­ã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯ãƒã‚¤ã‚ºãªã®ã§é™¤å¤–\n",
        "            all_texts.append(cleaned)\n",
        "            #print(f\"xtracted from: {fname}\")\n",
        "    except Exception as e:\n",
        "        print(f\"{fname}: {e}\")\n",
        "\n",
        "print(f\"\\næŠ½å‡ºã•ã‚ŒãŸè«–æ–‡æœ¬æ•°: {len(all_texts)}\")\n",
        "print(f\"ç·ãƒ†ã‚­ã‚¹ãƒˆé•·: {sum(len(t) for t in all_texts):,} æ–‡å­—\")\n",
        "\n",
        "# ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸€ã¤ã«ã¾ã¨ã‚ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ä¿å­˜ã—ã¦ãŠãï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰\n",
        "merged_text = \"\\n\\n\".join(all_texts)\n",
        "with open(\"merged_corpus.tex\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(merged_text)\n",
        "print(\"\\nmerged_corpus.tex ã«ä¿å­˜ã—ã¾ã—ãŸ\")"
      ],
      "metadata": {
        "id": "Dj8HIubk2O2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¨èªå½™è¾æ›¸ä½œæˆ"
      ],
      "metadata": {
        "id": "Hfxny7vSoFQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# SPECIAL_TOKENS ã¯ã•ã£ãã¨åŒã˜ã‚‚ã®ã‚’ä½¿ã†\n",
        "# SPECIAL_TOKENS = [\"<unk>\", \"<bos>\", \"<eos>\"]\n",
        "\n",
        "print(f\"ãƒˆãƒ¼ã‚¯ãƒ³ç·æ•°: {len(tokens):,}\")\n",
        "\n",
        "# èªå½™ã‚’é »åº¦ä¸Šä½ã ã‘ã«åœ§ç¸®\n",
        "counter = Counter(tokens)\n",
        "\n",
        "VOCAB_LIMIT = 40000\n",
        "most_common_tokens = [\n",
        "    w for (w, c) in counter.most_common(VOCAB_LIMIT)\n",
        "    if w not in SPECIAL_TOKENS\n",
        "]\n",
        "\n",
        "vocab = SPECIAL_TOKENS + most_common_tokens\n",
        "\n",
        "word2idx = {w: i for i, w in enumerate(vocab)}\n",
        "idx2word = {i: w for i, w in enumerate(vocab)}\n",
        "\n",
        "UNK_IDX = word2idx[\"<unk>\"]\n",
        "BOS_IDX = word2idx[\"<bos>\"]\n",
        "EOS_IDX = word2idx[\"<eos>\"]\n",
        "\n",
        "print(f\"èªå½™æ•°ï¼ˆåœ§ç¸®å¾Œï¼‰: {len(vocab):,}\")\n",
        "\n",
        "# ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‚’ index ã«å¤‰æ›\n",
        "indexed_tokens = [word2idx.get(t, UNK_IDX) for t in tokens]\n",
        "\n",
        "# å¿µã®ãŸã‚ãƒã‚§ãƒƒã‚¯\n",
        "max_id = max(indexed_tokens)\n",
        "print(\"æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³ID:\", max_id)\n",
        "print(\"èªå½™ã‚µã‚¤ã‚º:\", len(vocab))\n",
        "assert max_id < len(vocab), \"ãƒˆãƒ¼ã‚¯ãƒ³IDãŒèªå½™ã‚µã‚¤ã‚ºã‚’è¶…ãˆã¦ã„ã¾ã™ï¼\"\n"
      ],
      "metadata": {
        "id": "KHFzQd_2y9ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®šç¾©"
      ],
      "metadata": {
        "id": "SSNzsdoh7p20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Datasetã‚¯ãƒ©ã‚¹å®šç¾©"
      ],
      "metadata": {
        "id": "3EfH4t9CoW1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LatexDataset(Dataset):\n",
        "    \"\"\"å¤§ããªãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‹ã‚‰ã€ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒ«ã‚’åˆ‡ã‚Šå‡ºã™ Dataset\"\"\"\n",
        "    def __init__(self, data, seq_length=60, samples_per_epoch=3000):\n",
        "        self.data = data                      # â† ã™ã§ã« index åŒ–ã•ã‚ŒãŸåˆ—\n",
        "        self.seq_length = seq_length\n",
        "        self.samples_per_epoch = samples_per_epoch\n",
        "\n",
        "        self.max_start = len(self.data) - (self.seq_length + 1)\n",
        "        assert self.max_start > 0, \"ãƒ‡ãƒ¼ã‚¿ãŒçŸ­ã™ãã¾ã™\"\n",
        "\n",
        "    def __len__(self):\n",
        "        # 1epochã‚ãŸã‚Šã®ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’ç›´æ¥æ±ºã‚ã‚‹\n",
        "        return self.samples_per_epoch\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # idx ã¯ä½¿ã‚ãšã€æ¯å›ãƒ©ãƒ³ãƒ€ãƒ ãªä½ç½®ã‹ã‚‰åˆ‡ã‚Šå‡ºã™\n",
        "        start = random.randint(0, self.max_start)\n",
        "        end = start + self.seq_length + 1\n",
        "        chunk = self.data[start:end]\n",
        "\n",
        "        x = torch.tensor(chunk[:-1], dtype=torch.long)  # [T]\n",
        "        y = torch.tensor(chunk[1:], dtype=torch.long)   # [T]\n",
        "        return x, y\n"
      ],
      "metadata": {
        "id": "QEQH22RszXxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ã“ã“ã§åˆã‚ã¦ dataset ã‚’ä½œã‚‹ï¼ˆtokens ã§ã¯ãªã indexed_tokens ã‚’æ¸¡ã™ï¼‰\n",
        "seq_length = 60\n",
        "samples_per_epoch = 8000\n",
        "dataset = LatexDataset(indexed_tokens, seq_length=seq_length,\n",
        "                       samples_per_epoch=samples_per_epoch)\n"
      ],
      "metadata": {
        "id": "yQCAnpvHHnUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train / Validation ã«åˆ†å‰²ã—ã¦ DataLoader ã‚’ä½œæˆ"
      ],
      "metadata": {
        "id": "I9g7MOlx7G6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio = 0.9\n",
        "n_total = len(dataset)\n",
        "n_train = int(n_total * train_ratio)\n",
        "n_val = n_total - n_train\n",
        "\n",
        "train_ds, val_ds = random_split(dataset, [n_train, n_val])\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"ãƒ‡ãƒ¼ã‚¿æ•°: total={n_total}, train={len(train_ds)}, val={len(val_ds)}\")\n",
        "print(f\"1epoch ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆtrainï¼‰: {len(train_loader)}\")\n"
      ],
      "metadata": {
        "id": "vR0X-amZ64rH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### å­¦ç¿’"
      ],
      "metadata": {
        "id": "eHHlGpd272Ve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LSTMãƒ¢ãƒ‡ãƒ«å®šç¾©(ç–‘ä¼¼æ•°å­¦è«–æ–‡ç”Ÿæˆ)"
      ],
      "metadata": {
        "id": "6WpX1NvGqTO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTMãƒ¢ãƒ‡ãƒ«å®šç¾©(ç–‘ä¼¼æ•°å­¦è«–æ–‡ç”Ÿæˆ)\n",
        "class LatexTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    LaTeX ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ç”¨ã®ã‚·ãƒ³ãƒ—ãƒ«ãª Transformer è¨€èªãƒ¢ãƒ‡ãƒ«\n",
        "    å…¥åŠ›: x [B, T]  (ãƒˆãƒ¼ã‚¯ãƒ³ID)\n",
        "    å‡ºåŠ›: logits [B, T, V]  (å„ä½ç½®ã”ã¨ã®å˜èªåˆ†å¸ƒã®ãƒ­ã‚¸ãƒƒãƒˆ)\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 vocab_size: int,\n",
        "                 d_model: int = 256,          # åŸ‹ã‚è¾¼ã¿ï¼†éš ã‚Œæ¬¡å…ƒ\n",
        "                 nhead: int = 4,              # ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰æ³¨æ„ã®ãƒ˜ãƒƒãƒ‰æ•°\n",
        "                 num_layers: int = 4,         # Transformer å±¤æ•°\n",
        "                 dim_feedforward: int = 512,  # FFN ã®ä¸­é–“æ¬¡å…ƒ\n",
        "                 dropout: float = 0.1,\n",
        "                 max_seq_len: int = 512):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        # èªå½™åŸ‹ã‚è¾¼ã¿\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # ä½ç½®åŸ‹ã‚è¾¼ã¿ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãª learnable embeddingï¼‰\n",
        "        self.pos_embedding = nn.Embedding(max_seq_len, d_model)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True  # [B, T, E] ã§å‡¦ç†\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "\n",
        "        # å‡ºåŠ›å±¤\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "        # Causal mask ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨ã«ä¿æŒ\n",
        "        self.register_buffer(\"mask\", None, persistent=False)\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz: int, device: torch.device):\n",
        "        \"\"\"\n",
        "        è‡ªå·±å›å¸°ç”¨ã®ãƒã‚¹ã‚¯ (æœªæ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¦‹ãªã„ã‚ˆã†ã«ã™ã‚‹)\n",
        "        shape: [T, T]\n",
        "        \"\"\"\n",
        "        if (self.mask is None) or (self.mask.size(0) != sz):\n",
        "            mask = torch.full((sz, sz), float(\"-inf\"), device=device)\n",
        "            mask = torch.triu(mask, diagonal=1)\n",
        "            self.mask = mask\n",
        "        return self.mask\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: [B, T]  ãƒˆãƒ¼ã‚¯ãƒ³IDåˆ—\n",
        "        æˆ»ã‚Šå€¤:\n",
        "            logits: [B, T, V]\n",
        "        \"\"\"\n",
        "        B, T = x.size()\n",
        "        device = x.device\n",
        "\n",
        "        # ä½ç½®ID [0, 1, ..., T-1]\n",
        "        positions = torch.arange(T, device=device).unsqueeze(0).expand(B, T)  # [B, T]\n",
        "\n",
        "        tok_emb = self.token_embedding(x)          # [B, T, d_model]\n",
        "        pos_emb = self.pos_embedding(positions)    # [B, T, d_model]\n",
        "        h = tok_emb + pos_emb                      # [B, T, d_model]\n",
        "\n",
        "        # causal mask (æœªæ¥ã®æƒ…å ±ã‚’è¦‹ãªã„)\n",
        "        src_mask = self._generate_square_subsequent_mask(T, device=device)  # [T, T]\n",
        "\n",
        "        # TransformerEncoder ã«é€šã™\n",
        "        # batch_first=True ãªã®ã§ h: [B, T, d_model], mask ã¯ [T, T]\n",
        "        h = self.transformer(h, mask=src_mask)     # [B, T, d_model]\n",
        "\n",
        "        logits = self.fc_out(h)                    # [B, T, V]\n",
        "        return logits"
      ],
      "metadata": {
        "id": "h7m2BKKEobY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ç¹°ã‚Šè¿”ã—è¨ˆç®—"
      ],
      "metadata": {
        "id": "NKqEGskKqeEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"device: {device}\")\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print(\"vocab_size:\", vocab_size)\n",
        "\n",
        "model = LatexLSTM(vocab_size=vocab_size).to(device)\n",
        "\n",
        "print(\"model.fc.out_features:\", model.fc.out_features)\n",
        "assert model.fc.out_features == vocab_size, \"ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›æ¬¡å…ƒã¨èªå½™æ•°ãŒä¸€è‡´ã—ã¦ã„ã¾ã›ã‚“\"\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "num_epochs = 30\n",
        "max_grad_norm = 1.0  # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã®æœ€å¤§ãƒãƒ«ãƒ \n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # -------------------------------\n",
        "    # Train\n",
        "    # -------------------------------\n",
        "    model.train()\n",
        "    total_train_loss = 0.0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        # x, y: [B, T]\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)  # [B, T, V]\n",
        "\n",
        "        # CrossEntropyLoss ã¯ [N, C] vs [N] ã‚’ã¨ã‚‹ã®ã§ reshape ã™ã‚‹\n",
        "        loss = criterion(\n",
        "            logits.view(-1, logits.size(-1)),  # [B*T, V]\n",
        "            y.view(-1)                         # [B*T]\n",
        "        )\n",
        "        loss.backward()\n",
        "\n",
        "        # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆå‹¾é…çˆ†ç™ºå¯¾ç­–ï¼‰\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Validation\n",
        "    # -------------------------------\n",
        "    model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            val_loss = criterion(\n",
        "                logits.view(-1, logits.size(-1)),\n",
        "                y.view(-1)\n",
        "            )\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "    # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’é€²ã‚ã‚‹\n",
        "    scheduler.step()\n",
        "\n",
        "    print(\n",
        "        f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "        f\"train_loss={avg_train_loss:.4f}  val_loss={avg_val_loss:.4f}  \"\n",
        "        f\"lr={scheduler.get_last_lr()[0]:.5f}\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "mMzBAuxWqcea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### äºˆæ¸¬"
      ],
      "metadata": {
        "id": "5H6pGlea8C3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç”¨ãƒ˜ãƒ«ãƒ‘ãƒ¼\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fDyXRUFKolPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_from_logits(logits: torch.Tensor,\n",
        "                       temperature: float = 0.8,\n",
        "                       top_k: int = 50) -> int:\n",
        "    \"\"\"\n",
        "    logits: [V] ã®1æ™‚åˆ»åˆ†ã®ãƒ­ã‚¸ãƒƒãƒˆ\n",
        "    æˆ»ã‚Šå€¤: æ¬¡ã«é¸ã¶ãƒˆãƒ¼ã‚¯ãƒ³ID\n",
        "    \"\"\"\n",
        "    # æ¸©åº¦ã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
        "    logits = logits / temperature\n",
        "\n",
        "    # top-k ã ã‘æ®‹ã—ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆæ¥µç«¯ãªãƒã‚¤ã‚ºã‚’æ¸›ã‚‰ã™ï¼‰\n",
        "    if top_k is not None and top_k > 0:\n",
        "        values, indices = torch.topk(logits, top_k)\n",
        "        probs = torch.softmax(values, dim=-1)\n",
        "        idx_in_topk = torch.multinomial(probs, 1).item()\n",
        "        next_token_id = indices[idx_in_topk].item()\n",
        "        return next_token_id\n",
        "    else:\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        next_token_id = torch.multinomial(probs, 1).item()\n",
        "        return next_token_id\n"
      ],
      "metadata": {
        "id": "Rti0zCt5owOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆé–¢æ•°"
      ],
      "metadata": {
        "id": "A64q9CmRqu_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(seed_text, length=200, temperature=0.8, top_k=50, seq_length=60):\n",
        "    model.eval()\n",
        "    words = re.findall(TOKEN_PATTERN, seed_text)\n",
        "\n",
        "    for _ in range(length):\n",
        "        # ğŸ”¥ ç›´è¿‘ seq_length åˆ†ã ã‘ä½¿ã†\n",
        "        seq = [word2idx.get(w, UNK_IDX) for w in words[-seq_length:]]\n",
        "\n",
        "        x = torch.tensor([seq], dtype=torch.long).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)[0, -1]  # ãƒ©ã‚¹ãƒˆæ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ã®ã¿\n",
        "\n",
        "            # <unk> ã‚’å‡ºã«ããã™ã‚‹\n",
        "            logits[UNK_IDX] -= 10.0\n",
        "\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # top-k ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆæ¨å¥¨ï¼‰\n",
        "            values, indices = torch.topk(logits, top_k)\n",
        "            probs = torch.softmax(values, dim=0)\n",
        "            next_idx = indices[torch.multinomial(probs, 1).item()].item()\n",
        "\n",
        "        words.append(idx2word[next_idx])\n",
        "\n",
        "    return \" \".join(words)\n"
      ],
      "metadata": {
        "id": "j5K4ik0Bpa3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚µãƒ³ãƒ—ãƒ«"
      ],
      "metadata": {
        "id": "TUKHVRGPrHT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = r\"\\section{Introduction} We consider the problem of minimizing\"\n",
        "print(\"\\nç”Ÿæˆçµæœ:\\n\")\n",
        "text = generate(seed, length=200, temperature=0.7, top_k=30, seq_length=seq_length)\n",
        "print(text)"
      ],
      "metadata": {
        "id": "vcpQbqcTrAsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# LaTeX â†’ PDF å¤‰æ› (Google Colab / Linux ç’°å¢ƒç”¨)\n",
        "# =====================================================\n",
        "\n",
        "import os, subprocess\n",
        "\n",
        "# â‘  ç”Ÿæˆæ¸ˆã¿LaTeXæœ¬æ–‡ã‚’ã“ã“ã«è²¼ã‚‹ï¼ˆæ•´å½¢å¾Œã®textã‚’å…¥ã‚Œã‚‹ï¼‰\n",
        "latex_code = r\"\"\"\n",
        "\\documentclass[11pt]{article}\n",
        "\n",
        "% å¿…è¦æœ€ä½é™ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸\n",
        "\\usepackage{amsmath, amssymb, amsfonts}\n",
        "\\usepackage{bm}\n",
        "\\usepackage[margin=25mm]{geometry}\n",
        "\n",
        "\\begin{document}\n",
        "\n",
        "\\section{Introduction}\n",
        "\n",
        "We consider the problem of minimizing an approximation function under\n",
        "a general learning framework. In this work, we introduce a unified model\n",
        "that aims to improve prediction performance and enhance optimization stability\n",
        "across a variety of datasets.\n",
        "\n",
        "To study the theoretical properties of our model, we begin by examining\n",
        "a key lemma related to empirical risk minimization. A simplified form\n",
        "of the proposed inequality can be written as\n",
        "\\begin{equation}\n",
        "    \\mathcal{A}_{\\text{train}}(x)\n",
        "    \\leq \\mathcal{A}_{\\text{test}}(x) + \\epsilon,\n",
        "\\end{equation}\n",
        "where $\\epsilon$ is a small constant representing the empirical gap between\n",
        "training and test distributions.\n",
        "\n",
        "Furthermore, we consider the iterative update process of the model parameters.\n",
        "For example, the next state $\\mathbf{x}_{t+1}$ may be approximated by\n",
        "\\begin{equation}\n",
        "    \\mathbf{x}_{t+1}\n",
        "    = \\mathbf{x}_{t} - \\eta \\, \\nabla_{\\mathbf{x}} \\mathcal{L}(\\mathbf{x}_t),\n",
        "\\end{equation}\n",
        "where $\\eta$ is a learning rate and $\\mathcal{L}$ denotes the loss function.\n",
        "This provides a generic template for gradient-based optimization in our framework.\n",
        "\n",
        "The main contributions of this paper can be summarized as follows:\n",
        "\\begin{itemize}\n",
        "    \\item We propose a unified learning framework applicable to various tasks.\n",
        "    \\item We provide theoretical insights into the stability of the proposed model.\n",
        "    \\item We demonstrate competitive performance across multiple datasets.\n",
        "\\end{itemize}\n",
        "\n",
        "\\section{Conclusion}\n",
        "\n",
        "The generated LaTeX text has been reformatted so that it can be compiled\n",
        "without errors. Although the original automatically generated expressions\n",
        "were incomplete, we replaced them with valid representative examples\n",
        "to ensure successful PDF generation.\n",
        "\n",
        "\\end{document}\n",
        "\"\"\"\n",
        "\n",
        "# â‘¡ texãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜\n",
        "with open(\"generated.tex\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(latex_code)\n",
        "\n",
        "# â‘¢ LaTeXã‚³ãƒãƒ³ãƒ‰ãŒä½¿ãˆã‚‹ã‚ˆã†ã«TexLiveã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!apt-get update -qq\n",
        "!apt-get install -y texlive-latex-base texlive-latex-extra texlive-fonts-recommended > /dev/null\n",
        "\n",
        "# â‘£ pdflatexã§PDFã«å¤‰æ›\n",
        "!pdflatex -interaction=nonstopmode generated.tex > /dev/null\n",
        "\n",
        "# â‘¤ å‡ºåŠ›ç¢ºèª\n",
        "print(\"PDFç”Ÿæˆå®Œäº†: generated.pdf\")\n",
        "!ls -lh generated.pdf\n"
      ],
      "metadata": {
        "id": "jZMXytkVrvnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L12fyJHI-8KI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}