{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 11.9 Transformer\n",
        "è«–æ–‡ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã•ã›ã¦ãƒŠãƒ³ãƒãƒ£ãƒƒãƒ†è«–æ–‡ã‚’latexå½¢å¼ã§å‡ºåŠ›ã•ã›ã‚‹<br>\n",
        "Transformenrç‰ˆ"
      ],
      "metadata": {
        "id": "pVHUGT0PnVAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆ"
      ],
      "metadata": {
        "id": "SiCgS4G5nxHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets tqdm torch requests feedparser --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZf1dSfg2ncZ",
        "outputId": "bac9122d-46f4-4313-d8bd-2c366dde6552"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "doAWRTHmnT-E"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "import glob\n",
        "import re\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import urllib.parse\n",
        "import feedparser\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿"
      ],
      "metadata": {
        "id": "siqtNkzfn8NZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### arXiv API ã‹ã‚‰è«–æ–‡ ID ã‚’ã¾ã¨ã‚ã¦å–å¾—ã™ã‚‹é–¢æ•°"
      ],
      "metadata": {
        "id": "DmpfipBI3Gyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_arxiv_ids(category=\"cs.LG\", max_results=200):\n",
        "    \"\"\"\n",
        "    æŒ‡å®šã‚«ãƒ†ã‚´ãƒªã®æ–°ã—ã„ arXiv è«–æ–‡ ID ã‚’ã¾ã¨ã‚ã¦å–å¾—ã™ã‚‹ç°¡æ˜“é–¢æ•°\n",
        "    ä¾‹: category=\"cs.LG\"ï¼ˆMachine Learningï¼‰\n",
        "    \"\"\"\n",
        "    base_url = \"http://export.arxiv.org/api/query\"\n",
        "\n",
        "    # APIã®ã‚¯ã‚¨ãƒª: ã‚«ãƒ†ã‚´ãƒªæŒ‡å®š + æ–°ã—ã„é †\n",
        "    search_query = f\"cat:{category}\"\n",
        "    params = {\n",
        "        \"search_query\": search_query,\n",
        "        \"start\": 0,\n",
        "        \"max_results\": max_results,\n",
        "        \"sortBy\": \"submittedDate\",\n",
        "        \"sortOrder\": \"descending\",\n",
        "    }\n",
        "\n",
        "    url = base_url + \"?\" + urllib.parse.urlencode(params)\n",
        "    print(\"ğŸ” arXiv API URL:\", url)\n",
        "\n",
        "    feed = feedparser.parse(url)\n",
        "\n",
        "    paper_ids = []\n",
        "    for entry in feed.entries:\n",
        "        # entry.id ä¾‹: \"http://arxiv.org/abs/2401.12345v1\"\n",
        "        m = re.search(r'arxiv.org/abs/(\\d{4}\\.\\d+)', entry.id)\n",
        "        if m:\n",
        "            pid = m.group(1)  # \"2401.12345\"\n",
        "            paper_ids.append(pid)\n",
        "\n",
        "    print(f\"ğŸ“„ å–å¾—ã—ãŸè«–æ–‡IDæ•°: {len(paper_ids)} ä»¶\")\n",
        "    return paper_ids\n",
        "\n",
        "# ã“ã“ã§å¥½ããªã‚«ãƒ†ã‚´ãƒªãƒ»ä»¶æ•°ã‚’æŒ‡å®š\n",
        "# ä¾‹1: cs.LG (Machine Learning) æœ€æ–°ã‹ã‚‰200æœ¬\n",
        "# ä¾‹2: math.PR (Probability)\n",
        "paper_ids = fetch_arxiv_ids(category=\"cs.LG\", max_results=200)\n",
        "\n",
        "print(paper_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8z0Kiju93J5X",
        "outputId": "1de049f5-516e-448e-f8b0-e165e040a30d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” arXiv API URL: http://export.arxiv.org/api/query?search_query=cat%3Acs.LG&start=0&max_results=200&sortBy=submittedDate&sortOrder=descending\n",
            "ğŸ“„ å–å¾—ã—ãŸè«–æ–‡IDæ•°: 200 ä»¶\n",
            "['2511.21690', '2511.21689', '2511.21686', '2511.21678', '2511.21675', '2511.21669', '2511.21668', '2511.21667', '2511.21654', '2511.21652', '2511.21638', '2511.21635', '2511.21626', '2511.21622', '2511.21613', '2511.21607', '2511.21600', '2511.21594', '2511.21590', '2511.21581', '2511.21566', '2511.21561', '2511.21560', '2511.21550', '2511.21537', '2511.21531', '2511.21526', '2511.21514', '2511.21513', '2511.21500', '2511.21490', '2511.21474', '2511.21466', '2511.21465', '2511.21437', '2511.21416', '2511.21414', '2511.21408', '2511.21397', '2511.21381', '2511.21378', '2511.21377', '2511.21369', '2511.21364', '2511.21363', '2511.21356', '2511.21354', '2511.21350', '2511.21340', '2511.21338', '2511.21335', '2511.21320', '2511.21283', '2511.21276', '2511.21257', '2511.21247', '2511.21232', '2511.21223', '2511.21215', '2511.21213', '2511.21211', '2511.21208', '2511.21181', '2511.21140', '2511.21120', '2511.21118', '2511.21115', '2511.21109', '2511.21107', '2511.21104', '2511.21103', '2511.21101', '2511.21095', '2511.21092', '2511.21089', '2511.21088', '2511.21081', '2511.21080', '2511.21076', '2511.21075', '2511.21063', '2511.21056', '2511.21054', '2511.21050', '2511.21048', '2511.21040', '2511.21038', '2511.21035', '2511.21034', '2511.21032', '2511.21019', '2511.21016', '2511.21011', '2511.21009', '2511.21008', '2511.20997', '2511.20993', '2511.20992', '2511.20991', '2511.20987', '2511.20977', '2511.20974', '2511.20963', '2511.20960', '2511.20956', '2511.20944', '2511.20941', '2511.20934', '2511.20931', '2511.20927', '2511.20922', '2511.20913', '2511.20909', '2511.20893', '2511.20889', '2511.20888', '2511.20873', '2511.20871', '2511.20870', '2511.20853', '2511.20851', '2511.20849', '2511.20848', '2511.20844', '2511.20839', '2511.20836', '2511.20834', '2511.20830', '2511.20826', '2511.20823', '2511.20821', '2511.20814', '2511.20811', '2511.20804', '2511.20799', '2511.20798', '2511.20779', '2511.20643', '2511.20641', '2511.20640', '2511.20639', '2511.20636', '2511.20629', '2511.20626', '2511.20621', '2511.20613', '2511.20612', '2511.20609', '2511.20605', '2511.20604', '2511.20601', '2511.20597', '2511.20592', '2511.20591', '2511.20587', '2511.20586', '2511.20584', '2511.20577', '2511.20570', '2511.20564', '2511.20558', '2511.20544', '2511.20543', '2511.20541', '2511.20531', '2511.20516', '2511.20509', '2511.20503', '2511.20501', '2511.20500', '2511.20490', '2511.20489', '2511.20480', '2511.20478', '2511.20474', '2511.20469', '2511.20462', '2511.20457', '2511.20456', '2511.20445', '2511.20418', '2511.20407', '2511.20406', '2511.20397', '2511.20395', '2511.20382', '2511.20380', '2511.20362', '2511.20361', '2511.20349', '2511.20347', '2511.20333', '2511.20327', '2511.20315', '2511.20293', '2511.20283', '2511.20277', '2511.20273', '2511.20258', '2511.20257']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### arXiv ã‚½ãƒ¼ã‚¹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼†å±•é–‹"
      ],
      "metadata": {
        "id": "pFum5tb93p0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"papers\", exist_ok=True)\n",
        "\n",
        "for pid in tqdm(paper_ids, desc=\"Downloading\", ncols=80, ascii=True):\n",
        "    url = f\"https://arxiv.org/src/{pid}v1\"\n",
        "    out_path = f\"papers/{pid}.tar.gz\"\n",
        "    try:\n",
        "        r = requests.get(url, timeout=10)\n",
        "        if r.status_code == 200:\n",
        "            with open(out_path, \"wb\") as f:\n",
        "                f.write(r.content)\n",
        "            # tar / tar.gz / ãã®ä»–ã®åœ§ç¸®å½¢å¼ã«æŸ”è»Ÿã«å¯¾å¿œ\n",
        "            try:\n",
        "                with tarfile.open(out_path, \"r:*\") as tar:\n",
        "                    tar.extractall(f\"papers/{pid}\")\n",
        "            except tarfile.ReadError:\n",
        "                print(f\"Skipped {pid} (not a tar file)\")\n",
        "        else:\n",
        "            print(f\"Skipped {pid} (status {r.status_code})\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {pid}: {e}\")\n"
      ],
      "metadata": {
        "id": "K9mkCl0hnX9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93bf85f7-92ef-4520-99d6-0727836c8b00"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading:   0%|                                      | 0/200 [00:00<?, ?it/s]/tmp/ipython-input-2326405988.py:14: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(f\"papers/{pid}\")\n",
            "Downloading:  12%|###3                         | 23/200 [00:10<00:36,  4.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.21561 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:  38%|##########8                  | 75/200 [00:31<00:22,  5.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.21089 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:  44%|############9                | 89/200 [00:43<02:52,  1.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.21034 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:  46%|#############1               | 91/200 [00:44<01:34,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.21019 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:  48%|#############7               | 95/200 [00:45<00:48,  2.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.21009 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:  60%|################6           | 119/200 [00:58<00:27,  2.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.20871 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:  94%|##########################1 | 187/200 [02:12<00:05,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 2511.20382 (not a tar file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: 100%|############################| 200/200 [02:18<00:00,  1.44it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### çµæœç¢ºèª"
      ],
      "metadata": {
        "id": "uHy82Ndv4CZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!find papers -name \"*.tex\" | head -n 20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV_Im0bVwhlz",
        "outputId": "155955e1-535a-4bbb-e070-5727fda71a42"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "papers/2511.20406/imports.tex\n",
            "papers/2511.20406/log_2025.tex\n",
            "papers/2511.21550/content/intro.tex\n",
            "papers/2511.21550/content/heavy_momentum_mamba.tex\n",
            "papers/2511.21550/content/related_work.tex\n",
            "papers/2511.21550/content/_abstract.tex\n",
            "papers/2511.21550/content/result.tex\n",
            "papers/2511.21550/content/abstract.tex\n",
            "papers/2511.21550/content/har_system.tex\n",
            "papers/2511.21550/content/vanishing_gradient.tex\n",
            "papers/2511.21550/content/intro_.tex\n",
            "papers/2511.21550/content/prelim.tex\n",
            "papers/2511.21550/content/conclusion.tex\n",
            "papers/2511.21550/content/beyond.tex\n",
            "papers/2511.21550/content/har_system_.tex\n",
            "papers/2511.21550/content/prelim_.tex\n",
            "papers/2511.21550/content/continuous_momentum.tex\n",
            "papers/2511.21550/New_IEEEtran_how-to.tex\n",
            "papers/2511.21550/bare_jrnl_new_sample4.tex\n",
            "papers/2511.21550/math_commands.tex\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ãƒ‡ãƒ¼ã‚¿åŠ å·¥"
      ],
      "metadata": {
        "id": "IO3OjIdD7bRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LaTeX æœ¬æ–‡æŠ½å‡º ï¼‹ å‰å‡¦ç†"
      ],
      "metadata": {
        "id": "OSdfMXLInWPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re, glob\n",
        "\n",
        "def extract_latex_body(tex_text: str) -> str:\n",
        "    \"\"\"LaTeXæœ¬æ–‡ã‚’æŠ½å‡ºã—ã¦ã€æ•°å¼ã‚’å–ã‚Šé™¤ãã¤ã¤ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹ï¼ˆç°¡æ˜“ç‰ˆï¼‰\"\"\"\n",
        "    # ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã‚’å‰Šé™¤ï¼ˆ% ä»¥é™ï¼‰\n",
        "    text = re.sub(r'%.*', '', tex_text)\n",
        "\n",
        "    # \\begin{document}ã€œ\\end{document} ã®é–“ã ã‘ã‚’æŠœãå‡ºã—\n",
        "    m = re.search(r'\\\\begin{document}(.*?)\\\\end{document}', text, re.DOTALL)\n",
        "    if m:\n",
        "        text = m.group(1)\n",
        "\n",
        "    # bibliography / å‚è€ƒæ–‡çŒ®ãƒ»ä»˜éŒ²ä»¥é™ã‚’ã–ã£ãã‚Šå‰Šã‚‹\n",
        "    text = re.split(r'\\\\bibliography|\\\\begin{thebibliography}', text)[0]\n",
        "    text = re.split(r'\\\\appendix', text)[0]\n",
        "\n",
        "    # ==== ã“ã“ã‹ã‚‰æ•°å¼å‰Šé™¤ãƒ–ãƒ­ãƒƒã‚¯ ====\n",
        "\n",
        "    # ãƒ‡ã‚£ã‚¹ãƒ—ãƒ¬ã‚¤æ•°å¼ $$ ... $$\n",
        "    text = re.sub(r'\\$\\$(.+?)\\$\\$', ' ', text, flags=re.DOTALL)\n",
        "\n",
        "    # ãƒ‡ã‚£ã‚¹ãƒ—ãƒ¬ã‚¤æ•°å¼ \\[ ... \\]\n",
        "    text = re.sub(r'\\\\\\[(.+?)\\\\\\]', ' ', text, flags=re.DOTALL)\n",
        "\n",
        "    # ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³æ•°å¼ $ ... $\n",
        "    text = re.sub(r'\\$(.+?)\\$', ' ', text, flags=re.DOTALL)\n",
        "\n",
        "    # equation / align / gather / multline ç³»ã®ç’°å¢ƒã‚’å‰Šé™¤\n",
        "    math_envs = [\n",
        "        \"equation\", \"equation*\",\n",
        "        \"align\", \"align*\",\n",
        "        \"gather\", \"gather*\",\n",
        "        \"multline\", \"multline*\",\n",
        "    ]\n",
        "    for env in math_envs:\n",
        "        pattern = rf'\\\\begin{{{env}}}(.+?)\\\\end{{{env}}}'\n",
        "        text = re.sub(pattern, ' ', text, flags=re.DOTALL)\n",
        "\n",
        "    # ==== æ•°å¼å‰Šé™¤ã“ã“ã¾ã§ ====\n",
        "\n",
        "    # é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«ã¾ã¨ã‚ã‚‹\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "mRvNv8-k-I9L"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ã™ã¹ã¦ã® .tex ã‹ã‚‰æœ¬æ–‡ã‚’æŠ½å‡ºï¼ˆæ•°å¼ã¯é™¤å»æ¸ˆã¿ï¼‰\n",
        "all_texts = []\n",
        "for fname in glob.glob(\"papers/**/*.tex\", recursive=True):\n",
        "    try:\n",
        "        with open(fname, encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            raw = f.read()\n",
        "        cleaned = extract_latex_body(raw)\n",
        "        if len(cleaned) > 200:  # ã”ãçŸ­ã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯ãƒã‚¤ã‚ºãªã®ã§é™¤å¤–\n",
        "            all_texts.append(cleaned)\n",
        "            # print(f\"extracted from: {fname}\")\n",
        "    except Exception as e:\n",
        "        print(f\"{fname}: {e}\")\n",
        "\n",
        "print(f\"\\næŠ½å‡ºã•ã‚ŒãŸè«–æ–‡æœ¬æ•°: {len(all_texts)}\")\n",
        "print(f\"ç·ãƒ†ã‚­ã‚¹ãƒˆé•·: {sum(len(t) for t in all_texts):,} æ–‡å­—\")\n",
        "\n",
        "# ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸€ã¤ã«ã¾ã¨ã‚ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ä¿å­˜ã—ã¦ãŠãï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰\n",
        "merged_text = \"\\n\\n\".join(all_texts)\n",
        "with open(\"merged_corpus.tex\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(merged_text)\n",
        "print(\"\\nmerged_corpus.tex ã«ä¿å­˜ã—ã¾ã—ãŸ\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c275-ax-9bzn",
        "outputId": "d251fe5b-3398-4997-9e44-4c48868590ad"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "æŠ½å‡ºã•ã‚ŒãŸè«–æ–‡æœ¬æ•°: 860\n",
            "ç·ãƒ†ã‚­ã‚¹ãƒˆé•·: 9,338,060 æ–‡å­—\n",
            "\n",
            "merged_corpus.tex ã«ä¿å­˜ã—ã¾ã—ãŸ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¨èªå½™è¾æ›¸ä½œæˆ"
      ],
      "metadata": {
        "id": "Hfxny7vSoFQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã¨èªå½™æ§‹ç¯‰ ----\n",
        "from collections import Counter\n",
        "\n",
        "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºãƒ‘ã‚¿ãƒ¼ãƒ³\n",
        "# ï¼ˆLaTeXã‚³ãƒãƒ³ãƒ‰ã‚„è¨˜å·ã‚‚ä¸€å¿œãƒˆãƒ¼ã‚¯ãƒ³ã«ã—ã¦ã„ã¾ã™ãŒã€\n",
        "#  æ•°å¼æœ¬ä½“ã¯ä¸Šã® extract_latex_body ã§å‰Šé™¤æ¸ˆã¿ï¼‰\n",
        "TOKEN_PATTERN = r\"[A-Za-z]+|[.,;:!?()]\"\n",
        "SPECIAL_TOKENS = [\"<unk>\", \"<bos>\", \"<eos>\"]\n",
        "\n",
        "tokens = []\n",
        "for txt in all_texts:\n",
        "    # å„è«–æ–‡ã”ã¨ã« <bos>, <eos> ã‚’æŒŸã‚“ã§ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º\n",
        "    body_tokens = re.findall(TOKEN_PATTERN, txt)\n",
        "    tokens.extend([\"<bos>\"] + body_tokens + [\"<eos>\"])\n",
        "\n",
        "print(f\"ãƒˆãƒ¼ã‚¯ãƒ³ç·æ•°: {len(tokens):,}\")\n",
        "\n",
        "# èªå½™ã‚’é »åº¦ä¸Šä½ã ã‘ã«åœ§ç¸®\n",
        "counter = Counter(tokens)\n",
        "\n",
        "VOCAB_LIMIT = 10000\n",
        "most_common_tokens = [\n",
        "    w for (w, c) in counter.most_common(VOCAB_LIMIT)\n",
        "    if w not in SPECIAL_TOKENS\n",
        "]\n",
        "\n",
        "vocab = SPECIAL_TOKENS + most_common_tokens\n",
        "\n",
        "word2idx = {w: i for i, w in enumerate(vocab)}\n",
        "idx2word = {i: w for i, w in enumerate(vocab)}\n",
        "\n",
        "UNK_IDX = word2idx[\"<unk>\"]\n",
        "BOS_IDX = word2idx[\"<bos>\"]\n",
        "EOS_IDX = word2idx[\"<eos>\"]\n",
        "\n",
        "print(f\"èªå½™æ•°ï¼ˆåœ§ç¸®å¾Œï¼‰: {len(vocab):,}\")\n",
        "\n",
        "# ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‚’ index ã«å¤‰æ›\n",
        "indexed_tokens = [word2idx.get(t, UNK_IDX) for t in tokens]\n",
        "\n",
        "# å¿µã®ãŸã‚ãƒã‚§ãƒƒã‚¯\n",
        "max_id = max(indexed_tokens)\n",
        "print(\"æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³ID:\", max_id)\n",
        "print(\"èªå½™ã‚µã‚¤ã‚º:\", len(vocab))\n",
        "assert max_id < len(vocab), \"ãƒˆãƒ¼ã‚¯ãƒ³IDãŒèªå½™ã‚µã‚¤ã‚ºã‚’è¶…ãˆã¦ã„ã¾ã™ï¼\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHFzQd_2y9ux",
        "outputId": "15e37bf3-9600-4693-9d8f-be437f8ea89a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ãƒˆãƒ¼ã‚¯ãƒ³ç·æ•°: 1,548,730\n",
            "èªå½™æ•°ï¼ˆåœ§ç¸®å¾Œï¼‰: 10,001\n",
            "æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³ID: 10000\n",
            "èªå½™ã‚µã‚¤ã‚º: 10001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®šç¾©"
      ],
      "metadata": {
        "id": "SSNzsdoh7p20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Datasetã‚¯ãƒ©ã‚¹å®šç¾©"
      ],
      "metadata": {
        "id": "3EfH4t9CoW1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LatexDataset(Dataset):\n",
        "    \"\"\"å¤§ããªãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‹ã‚‰ã€ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒ«ã‚’åˆ‡ã‚Šå‡ºã™ Dataset\"\"\"\n",
        "    def __init__(self, data, seq_length=60, samples_per_epoch=3000):\n",
        "        self.data = data                      # â† ã™ã§ã« index åŒ–ã•ã‚ŒãŸåˆ—\n",
        "        self.seq_length = seq_length\n",
        "        self.samples_per_epoch = samples_per_epoch\n",
        "\n",
        "        self.max_start = len(self.data) - (self.seq_length + 1)\n",
        "        assert self.max_start > 0, \"ãƒ‡ãƒ¼ã‚¿ãŒçŸ­ã™ãã¾ã™\"\n",
        "\n",
        "    def __len__(self):\n",
        "        # 1epochã‚ãŸã‚Šã®ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’ç›´æ¥æ±ºã‚ã‚‹\n",
        "        return self.samples_per_epoch\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # idx ã¯ä½¿ã‚ãšã€æ¯å›ãƒ©ãƒ³ãƒ€ãƒ ãªä½ç½®ã‹ã‚‰åˆ‡ã‚Šå‡ºã™\n",
        "        start = random.randint(0, self.max_start)\n",
        "        end = start + self.seq_length + 1\n",
        "        chunk = self.data[start:end]\n",
        "\n",
        "        x = torch.tensor(chunk[:-1], dtype=torch.long)  # [T]\n",
        "        y = torch.tensor(chunk[1:], dtype=torch.long)   # [T]\n",
        "        return x, y\n"
      ],
      "metadata": {
        "id": "QEQH22RszXxJ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ã“ã“ã§åˆã‚ã¦ dataset ã‚’ä½œã‚‹ï¼ˆtokens ã§ã¯ãªã indexed_tokens ã‚’æ¸¡ã™ï¼‰\n",
        "seq_length = 128\n",
        "samples_per_epoch = 8000\n",
        "dataset = LatexDataset(indexed_tokens, seq_length=seq_length,\n",
        "                       samples_per_epoch=samples_per_epoch)\n"
      ],
      "metadata": {
        "id": "yQCAnpvHHnUu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train / Validation ã«åˆ†å‰²ã—ã¦ DataLoader ã‚’ä½œæˆ"
      ],
      "metadata": {
        "id": "I9g7MOlx7G6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio = 0.9\n",
        "n_total = len(dataset)\n",
        "n_train = int(n_total * train_ratio)\n",
        "n_val = n_total - n_train\n",
        "\n",
        "train_ds, val_ds = random_split(dataset, [n_train, n_val])\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"ãƒ‡ãƒ¼ã‚¿æ•°: total={n_total}, train={len(train_ds)}, val={len(val_ds)}\")\n",
        "print(f\"1epoch ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆtrainï¼‰: {len(train_loader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR0X-amZ64rH",
        "outputId": "13310736-fc9d-412f-ba87-63099d52e617"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ãƒ‡ãƒ¼ã‚¿æ•°: total=8000, train=7200, val=800\n",
            "1epoch ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆtrainï¼‰: 225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### å­¦ç¿’"
      ],
      "metadata": {
        "id": "eHHlGpd272Ve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Transformerãƒ¢ãƒ‡ãƒ«å®šç¾©(ç–‘ä¼¼è«–æ–‡ç”Ÿæˆ)"
      ],
      "metadata": {
        "id": "6WpX1NvGqTO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformerãƒ¢ãƒ‡ãƒ«å®šç¾©(ç–‘ä¼¼è«–æ–‡ç”Ÿæˆ)\n",
        "class LatexTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    LaTeX ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ç”¨ã®ã‚·ãƒ³ãƒ—ãƒ«ãª Transformer è¨€èªãƒ¢ãƒ‡ãƒ«\n",
        "    å…¥åŠ›: x [B, T]  (ãƒˆãƒ¼ã‚¯ãƒ³ID)\n",
        "    å‡ºåŠ›: logits [B, T, V]  (å„ä½ç½®ã”ã¨ã®å˜èªåˆ†å¸ƒã®ãƒ­ã‚¸ãƒƒãƒˆ)\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 vocab_size: int,\n",
        "                 d_model: int = 384,\n",
        "                 nhead: int = 6,\n",
        "                 num_layers: int = 6,\n",
        "                 dim_feedforward: int = 1024,\n",
        "                 dropout: float = 0.1,\n",
        "                 max_seq_len: int = 512):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_len = max_seq_len\n",
        "        # èªå½™åŸ‹ã‚è¾¼ã¿\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        # ä½ç½®åŸ‹ã‚è¾¼ã¿ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãª learnable embeddingï¼‰\n",
        "        self.pos_embedding = nn.Embedding(max_seq_len, d_model)\n",
        "        # Transformer Encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True  # [B, T, E] ã§å‡¦ç†\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        # å‡ºåŠ›å±¤\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        # Causal mask ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨ã«ä¿æŒ\n",
        "        self.register_buffer(\"mask\", None, persistent=False)\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz: int, device: torch.device):\n",
        "        \"\"\"\n",
        "        è‡ªå·±å›å¸°ç”¨ã®ãƒã‚¹ã‚¯ (æœªæ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¦‹ãªã„ã‚ˆã†ã«ã™ã‚‹)\n",
        "        shape: [T, T]\n",
        "        \"\"\"\n",
        "        if (self.mask is None) or (self.mask.size(0) != sz):\n",
        "            mask = torch.full((sz, sz), float(\"-inf\"), device=device)\n",
        "            mask = torch.triu(mask, diagonal=1)\n",
        "            self.mask = mask\n",
        "        return self.mask\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: [B, T]  ãƒˆãƒ¼ã‚¯ãƒ³IDåˆ—\n",
        "        æˆ»ã‚Šå€¤:\n",
        "            logits: [B, T, V]\n",
        "        \"\"\"\n",
        "        B, T = x.size()\n",
        "        device = x.device\n",
        "        # ä½ç½®ID [0, 1, ..., T-1]\n",
        "        positions = torch.arange(T, device=device).unsqueeze(0).expand(B, T)  # [B, T]\n",
        "        tok_emb = self.token_embedding(x)          # [B, T, d_model]\n",
        "        pos_emb = self.pos_embedding(positions)    # [B, T, d_model]\n",
        "        h = tok_emb + pos_emb                      # [B, T, d_model]\n",
        "        # causal mask (æœªæ¥ã®æƒ…å ±ã‚’è¦‹ãªã„)\n",
        "        src_mask = self._generate_square_subsequent_mask(T, device=device)  # [T, T]\n",
        "        # TransformerEncoder ã«é€šã™\n",
        "        # batch_first=True ãªã®ã§ h: [B, T, d_model], mask ã¯ [T, T]\n",
        "        h = self.transformer(h, mask=src_mask)     # [B, T, d_model]\n",
        "        logits = self.fc_out(h)                    # [B, T, V]\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "h7m2BKKEobY6"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ç¹°ã‚Šè¿”ã—è¨ˆç®—"
      ],
      "metadata": {
        "id": "NKqEGskKqeEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"device: {device}\")\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "model = LatexTransformer(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=256,          # 384 â†’ 256\n",
        "    nhead=4,              # 6 â†’ 4\n",
        "    num_layers=4,         # 6 â†’ 4\n",
        "    dim_feedforward=768,  # 1024 â†’ 768\n",
        "    dropout=0.1,\n",
        "    max_seq_len=512\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "num_epochs = 30\n",
        "max_grad_norm = 1.0  # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã®æœ€å¤§ãƒãƒ«ãƒ \n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # -------------------------------\n",
        "    # Train\n",
        "    # -------------------------------\n",
        "    model.train()\n",
        "    total_train_loss = 0.0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        # x, y: [B, T]\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)  # [B, T, V]\n",
        "\n",
        "        # CrossEntropyLoss ã¯ [N, C] vs [N] ã‚’ã¨ã‚‹ã®ã§ reshape ã™ã‚‹\n",
        "        loss = criterion(\n",
        "            logits.view(-1, logits.size(-1)),  # [B*T, V]\n",
        "            y.view(-1)                         # [B*T]\n",
        "        )\n",
        "        loss.backward()\n",
        "\n",
        "        # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆå‹¾é…çˆ†ç™ºå¯¾ç­–ï¼‰\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Validation\n",
        "    # -------------------------------\n",
        "    model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            val_loss = criterion(\n",
        "                logits.view(-1, logits.size(-1)),\n",
        "                y.view(-1)\n",
        "            )\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "    # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’é€²ã‚ã‚‹\n",
        "    scheduler.step()\n",
        "\n",
        "    print(\n",
        "        f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "        f\"train_loss={avg_train_loss:.4f}  val_loss={avg_val_loss:.4f}  \"\n",
        "        f\"lr={scheduler.get_last_lr()[0]:.5f}\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMzBAuxWqcea",
        "outputId": "a5db6bad-4fd5-4ff1-8776-7f7d7376ef0c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n",
            "Epoch [1/30] train_loss=6.4095  val_loss=5.6767  lr=0.00100\n",
            "Epoch [2/30] train_loss=5.3215  val_loss=5.0340  lr=0.00100\n",
            "Epoch [3/30] train_loss=4.8536  val_loss=4.6585  lr=0.00100\n",
            "Epoch [4/30] train_loss=4.5790  val_loss=4.3923  lr=0.00100\n",
            "Epoch [5/30] train_loss=4.3778  val_loss=4.2029  lr=0.00100\n",
            "Epoch [6/30] train_loss=4.2348  val_loss=4.0388  lr=0.00100\n",
            "Epoch [7/30] train_loss=4.0965  val_loss=4.0069  lr=0.00100\n",
            "Epoch [8/30] train_loss=3.9862  val_loss=3.8245  lr=0.00100\n",
            "Epoch [9/30] train_loss=3.8949  val_loss=3.7119  lr=0.00100\n",
            "Epoch [10/30] train_loss=3.7974  val_loss=3.6445  lr=0.00050\n",
            "Epoch [11/30] train_loss=3.6699  val_loss=3.4911  lr=0.00050\n",
            "Epoch [12/30] train_loss=3.5991  val_loss=3.4043  lr=0.00050\n",
            "Epoch [13/30] train_loss=3.5681  val_loss=3.4186  lr=0.00050\n",
            "Epoch [14/30] train_loss=3.5109  val_loss=3.3464  lr=0.00050\n",
            "Epoch [15/30] train_loss=3.4644  val_loss=3.2962  lr=0.00050\n",
            "Epoch [16/30] train_loss=3.4482  val_loss=3.2757  lr=0.00050\n",
            "Epoch [17/30] train_loss=3.4113  val_loss=3.1986  lr=0.00050\n",
            "Epoch [18/30] train_loss=3.3644  val_loss=3.2130  lr=0.00050\n",
            "Epoch [19/30] train_loss=3.3336  val_loss=3.1308  lr=0.00050\n",
            "Epoch [20/30] train_loss=3.2988  val_loss=3.0973  lr=0.00025\n",
            "Epoch [21/30] train_loss=3.2623  val_loss=3.0906  lr=0.00025\n",
            "Epoch [22/30] train_loss=3.2206  val_loss=3.0384  lr=0.00025\n",
            "Epoch [23/30] train_loss=3.2103  val_loss=2.9461  lr=0.00025\n",
            "Epoch [24/30] train_loss=3.1731  val_loss=2.9850  lr=0.00025\n",
            "Epoch [25/30] train_loss=3.1873  val_loss=2.9888  lr=0.00025\n",
            "Epoch [26/30] train_loss=3.1482  val_loss=2.9688  lr=0.00025\n",
            "Epoch [27/30] train_loss=3.1529  val_loss=2.9205  lr=0.00025\n",
            "Epoch [28/30] train_loss=3.1105  val_loss=2.8970  lr=0.00025\n",
            "Epoch [29/30] train_loss=3.1129  val_loss=2.8882  lr=0.00025\n",
            "Epoch [30/30] train_loss=3.0927  val_loss=2.8925  lr=0.00013\n",
            "CPU times: user 6min 18s, sys: 6.68 s, total: 6min 24s\n",
            "Wall time: 6min 32s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### äºˆæ¸¬"
      ],
      "metadata": {
        "id": "5H6pGlea8C3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç”¨ãƒ˜ãƒ«ãƒ‘ãƒ¼\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fDyXRUFKolPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_from_logits(logits: torch.Tensor,\n",
        "                       temperature: float = 0.8,\n",
        "                       top_k: int = 50) -> int:\n",
        "    \"\"\"\n",
        "    logits: [V] ã®1æ™‚åˆ»åˆ†ã®ãƒ­ã‚¸ãƒƒãƒˆ\n",
        "    æˆ»ã‚Šå€¤: æ¬¡ã«é¸ã¶ãƒˆãƒ¼ã‚¯ãƒ³ID\n",
        "    \"\"\"\n",
        "    # æ¸©åº¦ã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
        "    logits = logits / temperature\n",
        "\n",
        "    # top-k ã ã‘æ®‹ã—ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆæ¥µç«¯ãªãƒã‚¤ã‚ºã‚’æ¸›ã‚‰ã™ï¼‰\n",
        "    if top_k is not None and top_k > 0:\n",
        "        values, indices = torch.topk(logits, top_k)\n",
        "        probs = torch.softmax(values, dim=-1)\n",
        "        idx_in_topk = torch.multinomial(probs, 1).item()\n",
        "        next_token_id = indices[idx_in_topk].item()\n",
        "        return next_token_id\n",
        "    else:\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        next_token_id = torch.multinomial(probs, 1).item()\n",
        "        return next_token_id\n"
      ],
      "metadata": {
        "id": "Rti0zCt5owOo"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆé–¢æ•°"
      ],
      "metadata": {
        "id": "A64q9CmRqu_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(seed_text, length=200, temperature=0.8, top_k=50,\n",
        "             seq_length=60, repetition_penalty=1.2, penalty_window=50,\n",
        "             math_penalty=3.0):\n",
        "\n",
        "    model.eval()\n",
        "    words = re.findall(TOKEN_PATTERN, seed_text)\n",
        "\n",
        "    # æ•°å¼é–¢é€£ãƒ¯ãƒ¼ãƒ‰ï¼ˆæŠ‘åˆ¶å¯¾è±¡ï¼‰\n",
        "    MATH_TOKENS = [\"begin\", \"align\", \"frac\", \"theta\", \"exp\", \"sum\", \"geq\",\n",
        "               \"cdot\", \"Bigg\", \"Pr\", \"Var\", \"sim\", \"=\",\"_\",\"^\",\n",
        "               \"ref\", \"eqref\", \"cite\", \"citet\", \"apdx\", \"section\", \"Fig\"]\n",
        "\n",
        "    for _ in range(length):\n",
        "        seq = [word2idx.get(w, UNK_IDX) for w in words[-seq_length:]]\n",
        "        x = torch.tensor([seq], dtype=torch.long).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)[0, -1]\n",
        "\n",
        "            logits[UNK_IDX] -= 10.0  # <unk> æ¸›å°‘\n",
        "\n",
        "            # ğŸ”¥ æ•°å¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŠ‘åˆ¶\n",
        "            for tk in MATH_TOKENS:\n",
        "                if tk in word2idx:\n",
        "                    logits[word2idx[tk]] -= math_penalty\n",
        "\n",
        "            # ğŸ” ç¹°ã‚Šè¿”ã—æŠ‘åˆ¶\n",
        "            recent = seq[-penalty_window:]\n",
        "            for tid in set(recent):\n",
        "                logits[tid] /= repetition_penalty\n",
        "\n",
        "            logits = logits / temperature\n",
        "\n",
        "            values, indices = torch.topk(logits, top_k)\n",
        "            probs = torch.softmax(values, dim=0)\n",
        "            next_idx = indices[torch.multinomial(probs, 1).item()].item()\n",
        "\n",
        "        words.append(idx2word[next_idx])\n",
        "\n",
        "    return \" \".join(words)\n"
      ],
      "metadata": {
        "id": "j5K4ik0Bpa3V"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚µãƒ³ãƒ—ãƒ«"
      ],
      "metadata": {
        "id": "TUKHVRGPrHT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = (\n",
        "    \"In this paper, we explore a novel approach to improving model \"\n",
        "    \"robustness under changing data distributions. \"\n",
        "    \"Our method is motivated by the observation that\"\n",
        ")\n",
        "print(\"\\nç”Ÿæˆçµæœ:\\n\")\n",
        "text = generate(\n",
        "    seed_text=seed,\n",
        "    length=280,\n",
        "    temperature=0.6,\n",
        "    top_k=15,\n",
        "    repetition_penalty=1.3,\n",
        "    math_penalty=6.0\n",
        ")\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcpQbqcTrAsa",
        "outputId": "7fa9edc4-f38b-4921-c39c-64a7fa1dbfdc"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ç”Ÿæˆçµæœ:\n",
            "\n",
            "In this paper , we explore a novel approach to improving model robustness under changing data distributions . Our method is motivated by the observation that effectively prioritizes training and inference time constraints . The proposed method can be viewed as a framework for unsupervised anomaly detection . By leveraging GMM adapts ensemble performance in terms of ranking tasks , we demonstrate why this approach is robust to its effectiveness on improving model robustness by explicitly aligning the reconstruction quality and overall accuracy with improved robustness while retaining both nDCG and nDCG capacity . This suggests that our method effectively prevents interpretability from cross modal feature retention under extreme imbalance , where models exhibit stronger discrimination ( Figure ref fig : intro toy experiments results for further ablation study ) to reduce the number of modalities . subsection Ablation Study label sec : ablations The effectiveness of this work is that our method consistently improves robustness and generalization ability with a single modality fusion mechanism , particularly when integrated into existing methods are comparable across multiple seeds . For instance , they often suffer from noise characteristics cite wu distribution balanced , chen class , xia lmpt , yang generalized , jiang faster , guo long tailed and multimodal learning for multi label visual recognition tasks while leveraging VLMs like CLIP citep radford clip , BLIP VQA . looseness While these methods are effective in prior work has explored the use of recent advances in cite xia lmpt , they have demonstrated promising advantages to tackle long tailed imbalances within image text rich multi label visual recognition tasks such as LMPT cite feng learning and deep generative modeling . However , existing methods typically rely on manually engineered features or lack their representations for downstream applications . In contrast , our approach leverages coarse grained user\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hUDGr5RGu7OM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Â ç”ŸæˆAIç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
        "ã“ã®ç”Ÿæˆã•ã‚ŒãŸè‹±æ–‡ã‚’ã€è«–æ–‡ã®ã€ŒRelated Workã€ã¨ã—ã¦è‡ªç„¶ãªè‹±èªã«æ•´å½¢ã—ã¦ãã ã•ã„ã€‚ LaTeX æ§‹æ–‡ï¼ˆsection, cite ãªã©ï¼‰ã¯å‰Šé™¤ã—ã€æ–‡æ³•ã®ä¸è‡ªç„¶ãªéƒ¨åˆ†ã‚’ä¿®æ­£ã—ã€ æ„å‘³ãŒé€šã‚‹ã¾ã¨ã¾ã£ãŸæ®µè½ï¼ˆ100ã€œ150 wordsï¼‰ã«ã—ã¦ãã ã•ã„ã€‚"
      ],
      "metadata": {
        "id": "u1T_9PbAuthH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_related_work = (\n",
        "    \"Our work introduces a novel approach to enhancing model robustness against evolving data distributions, \"\n",
        "    \"designed to optimize both training and inference efficiency. This method effectively addresses \"\n",
        "    \"interpretability challenges stemming from cross-modal feature retention, particularly in scenarios \"\n",
        "    \"of extreme data imbalance. While prior research has investigated methods for tackling long-tailed \"\n",
        "    \"imbalances in multimodal learning, leveraging techniques such as LMPT and deep generative modeling, \"\n",
        "    \"these often depend on hand-engineered features or struggle with robust representation learning. We \"\n",
        "    \"show that our approach not only consistently improves robustness and generalization capabilities \"\n",
        "    \"through a unified modality fusion mechanism but also performs well in unsupervised anomaly \"\n",
        "    \"detection, distinguishing itself from existing models that frequently suffer from noise \"\n",
        "    \"sensitivity and struggle to maintain performance across varied datasets.\"\n",
        ")\n",
        "print(formatted_related_work)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZMXytkVrvnL",
        "outputId": "412c2c80-b4a6-45c2-b4c2-33ab03a84653"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our work introduces a novel approach to enhancing model robustness against evolving data distributions, designed to optimize both training and inference efficiency. This method effectively addresses interpretability challenges stemming from cross-modal feature retention, particularly in scenarios of extreme data imbalance. While prior research has investigated methods for tackling long-tailed imbalances in multimodal learning, leveraging techniques such as LMPT and deep generative modeling, these often depend on hand-engineered features or struggle with robust representation learning. We show that our approach not only consistently improves robustness and generalization capabilities through a unified modality fusion mechanism but also performs well in unsupervised anomaly detection, distinguishing itself from existing models that frequently suffer from noise sensitivity and struggle to maintain performance across varied datasets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L12fyJHI-8KI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}